---
title: "Meta_summary_stats"
output: html_document
date: "2023-11-13"
editor_options: 
  chunk_output_type: console
---

The purpose of this script is to investigate the structure and makeup of the data that we have from the studies that are included. I want to spit out histograms that will show how much of each study design we have, how many fall into different climates and time since fire buckets

# Status: in progress

# ==============================================================================
# Author: Jake Cavaiani; jake.cavaiani@pnnl.gov
# 13 November 2023
# ==============================================================================



```{r Load packages}
#for Jake/mac

rm(list=ls(all=T)) #this clears your Environment

library(ggplot2)
library(googlesheets4)
library(tidyverse)
library(lubridate)
library(ggpubr)
library(here)
library(corrplot)
library(PerformanceAnalytics)
library(Hmisc)
library(GGally)
```

# ============================= Landcover types ========================================
```{r}
# Landcover data first 
landcover_data <- read_csv(here("Output_for_analysis", "06_Meta_geospatial_extraction_with_comids",
                                 "random_forest_data.csv")) %>%
  mutate(Site = str_replace_all(Site, "_", " ")) %>% 
  dplyr::select(Site, comid, PctOw2001:PctHbWet2019)

landcover_long <- landcover_data %>%
  pivot_longer(
    cols = starts_with("Pct"), 
    names_to = "landcover_type", 
    values_to = "percent_landcover") %>%
    mutate(Vegetation_year_pull = str_extract(landcover_type, "\\d{4}")) 

# NLCD2001 # We are running this for every year that it has. We are going to run mid slope 
  # PctOw2001Ws - Open Water Percentage
  # PctIce2001Ws - Ice/Snow Cover Percentage
  # PctUrbOp2001Ws - Developed, Open Space Land Use Percentage
  # PctUrbLo2001Ws - Developed, Low Intensity Land Use Percentage
  # PctUrbMd2001Ws - Developed, Medium Intensity Land Use Percentage
  # PctUrbHi2001Ws - Developed, High Intensity Land Use Percentage
  # PctBl2001Ws - Bedrock and Similar Earthen Material Percentage
  # PctDecid2001Ws - Deciduous Forest Percentage
  # PctConif2001Ws - Evergreen Forest Percentage
  # PctMxFst2001Ws - Mixed Deciduous/Evergreen Forest Percentage
  # PctShrb2001Ws - Shrub/Scrub Percentage
  # PctGrs2001Ws - Grassland/Herbaceous Percentage
  # PctHay2001Ws - Pasture/Hay Percentage
  # PctCrop2001Ws - Row Crop Percentage
  # PctWdWet2001Ws - Woody Wetland Percentage
  # PctHbWet2001Ws - Herbaceous Wetland Percentage 

# I can merge a few of them together and then will I just be adding?
# Forest
  # PctDecid2001Ws
  # PctConif2001Ws
  # PctMxFst2001Ws
# Urban
  # PctUrbOp2001Ws
  # PctUrbLo2001Ws
  # PctUrbMd2001Ws
  # PctUrbHi2001Ws
# Grassland
  # PctShrb2001Ws
  # PctGrs2001Ws
# Wetland
  # PctWdWet2001Ws
  # PctHbWet2001Ws
# AG
  # PctHay2001Ws
  # PctCrop2001Ws

# Forest
forest_summary <- landcover_long %>%
  filter(str_starts(landcover_type, "PctDecid") | str_starts(landcover_type, "PctConif") | 
         str_starts(landcover_type, "PctMxFst")) %>%
  group_by(Site, Vegetation_year_pull) %>%
  summarise(Forest = sum(percent_landcover, na.rm = TRUE)) 

# Urban
urban_summary <- landcover_long %>% 
  filter(str_starts(landcover_type, "PctUrbOp") | str_starts(landcover_type, "PctUrbLo") | 
         str_starts(landcover_type, "PctUrbMd") | str_starts(landcover_type, "PctUrbHi")) %>%
  group_by(Site, Vegetation_year_pull) %>%
  summarise(Urban = sum(percent_landcover, na.rm = TRUE))

# Grassland 
grassland_summary <- landcover_long %>%
  filter(str_starts(landcover_type, "PctShrb") | str_starts(landcover_type, "PctGrs")) %>%
  group_by(Site, Vegetation_year_pull) %>%
  summarise(Grassland = sum(percent_landcover, na.rm = TRUE))

# Wetland
wetland_summary <- landcover_long %>%
  filter(str_starts(landcover_type, "PctWdWet") | str_starts(landcover_type, "PctHbWet")) %>%
  group_by(Site, Vegetation_year_pull) %>%
  summarise(Wetland = sum(percent_landcover, na.rm = TRUE))

# AG
AG_summary <- landcover_long %>%
  filter(str_starts(landcover_type, "PctHay") | str_starts(landcover_type, "PctCrop")) %>%
  group_by(Site, Vegetation_year_pull) %>%
  summarise(AG = sum(percent_landcover, na.rm = TRUE))

# Merge the Forest and Wetland summaries back to the original data frame
landcover_summary <- landcover_long %>%
  left_join(forest_summary, by = c("Site", "Vegetation_year_pull")) %>%
  left_join(urban_summary, by = c("Site", "Vegetation_year_pull")) %>% 
  left_join(grassland_summary, by = c("Site", "Vegetation_year_pull")) %>% 
  left_join(wetland_summary, by = c("Site", "Vegetation_year_pull")) %>% 
  left_join(AG_summary, by = c("Site", "Vegetation_year_pull"))

landcover_summary <- landcover_summary %>% 
  dplyr::select(!landcover_type:percent_landcover) %>%
  distinct(Vegetation_year_pull, Site, .keep_all = TRUE)

```

# ============================= Build random forest dataframe ==================================== 
### Load in geospatial data 
```{r}
# The effect sizes of each study's watershed
effect_size <- read_csv(here("Output_for_analysis", "04_Meta_merge_all_studies_effect_size", "Effect_Size.csv"), na = c('-9999', 'N/A')) %>% 
  dplyr::select(Study_ID, response_var, Climate, Biome, Effect_size, Effect_size_pair, Time_Since_Fire)

# This has site and effect size pair which will merge with the effect size dataframe to get a site name with each effect size.
study_site_data <- read_csv(here("inputs", "catchment_characteristics", "Fire_name_Lat_Long.csv"), na = c('-9999', 'N/A')) %>%
  dplyr::select(Study_ID, Site, Fire_year, Vegetation_year_pull, Effect_size_pair, burn_percentage) %>%
  rename(year = Fire_year) %>%
  mutate(Site = str_replace_all(Site, "_", " "))

no_geo_random_forest_data <- full_join(effect_size, study_site_data) %>%
  na.omit(Effect_size) # this na.omit eliminates all the control sites that dont have an associated effect size.

unique(no_geo_random_forest_data$Site) # Supposed to be 28 sites here

landcover_data_merged <- left_join(no_geo_random_forest_data, landcover_summary) %>% 
  na.omit(percent_landcover) %>% 
  arrange(Vegetation_year_pull)

# Now I want to read in the rest of the geospatial data to merge with the above data frame. 
# Geospatial data 
geospatial_data <- read_csv(here("Output_for_analysis", "06_Meta_geospatial_extraction_with_comids",
                                 "random_forest_data.csv")) %>%
  mutate(Site = str_replace_all(Site, "_", " ")) %>% 
  dplyr::select(!MTBS_1986Ws:MTBS_2017Ws) %>% # Pulled in from study_site_data
  dplyr::select(!PctOw2001:PctHbWet2019)


random_forest_data <- full_join(landcover_data_merged, geospatial_data) %>% 
  na.omit(Effect_size)

# Now add in the burn severity 
DNBR_Severity <- read_csv(here("Output_for_analysis", "14_Meta_calculate_burn_severity", "DNBR_Severity.csv")) %>% 
  dplyr::select(Site, mean_dnbr, burn_severity_category)

random_forest_data <- left_join(random_forest_data, DNBR_Severity, by = "Site")


unique(random_forest_data$Site) # supposed to be 24 sites here 

# removing duplicates
random_forest_data <- random_forest_data %>% 
  group_by(Site) %>% 
  distinct() %>% 
  ungroup()

#splitting by analyte
doc_RF <- random_forest_data %>%
  filter(response_var == "DOC_Interp")

write_csv(doc_RF, here("Output_for_analysis", "08_Meta_summary_stats", "doc_meta_random_forest.csv"))

no3_RF <- random_forest_data %>%
  filter(response_var == "NO3_Interp") 

write_csv(no3_RF, here("Output_for_analysis", "08_Meta_summary_stats", "no3_meta_random_forest.csv"))

```

# ========================== Exploratory data frames ==================================
```{r}
TSF_DNBR_DOC <- doc_RF %>% 
  dplyr::select(Site, mean_dnbr, Time_Since_Fire)


```


# ============================= Correlation Matrix ==================================== 
# DOC
```{r correlation plot}
# Simple correlation matrix between effect size and all the other catchment characteristics
doc_correlation <- doc_RF %>% 
  dplyr::select(Effect_size, totdasqkm, minelevraw, maxelevsmo, minelevsmo, elevfixed, slope, Tmax9120Ws,Tmean9120Ws, Tmin9120Ws, Forest, Urban, Grassland, Wetland, AG, OmWs, BFIWs, PermWs, RunoffWs, burn_percentage, mean_dnbr, Time_Since_Fire) %>% 
  mutate(Time_Since_Fire = as.numeric(Time_Since_Fire)) 

# If mean DNBR values are calculated then our measurement is no bueno. Remove. 
doc_correlation <- doc_correlation %>% 
  mutate(mean_dnbr = case_when(mean_dnbr < 0 ~ NA,
                               TRUE ~ mean_dnbr))

# Calculate the correlation matrix
cor_matrix_doc <- cor(doc_correlation, use = "complete.obs")

# Assuming cor_matrix_doc is your correlation matrix
cor_matrix_doc <- as.data.frame(cor_matrix_doc)

# Get the order of columns with 'Effect_size' first
column_order <- c("Effect_size", setdiff(names(cor_matrix_doc), "Effect_size"))

# Reorder columns using select
cor_matrix_doc <- cor_matrix_doc %>%
  dplyr::select(all_of(column_order))

# Reorder rows by converting row names to a column, ordering, then converting back
cor_matrix_doc <- cor_matrix_doc %>%
  rownames_to_column(var = "rowname") %>%
  arrange(factor(rowname, levels = column_order)) %>%
  column_to_rownames(var = "rowname")

# Convert back to matrix if needed
cor_matrix_doc <- as.matrix(cor_matrix_doc)

# Print the reordered correlation matrix
print(cor_matrix_doc)

# # Filter the correlation matrix to include only values greater than 0.6
cor_matrix_filtered_doc <- ifelse(abs(cor_matrix_doc) > 0.1, cor_matrix_doc, NA)

# Specify the file path and format for the output PDF file
output_file <- here("initial_plots", "08_Meta_summary_stats", "DOC_geospatial_correlation.pdf")

# Open a PDF device
pdf(file = output_file, width = 12, height = 12)

# Plot the reordered correlation matrix using corrplot
corrplot(cor_matrix_doc, method = "circle", type = "upper", order = "original", 
         tl.cex = 0.8, tl.col = "black", addgrid.col = "grey")

# Add a title to the bottom left corner
mtext("DOC Correlation Matrix", side = 3, line = 1, adj = 0, cex = 1.2)


# Close the device to save the file
dev.off()


```

# NO3
```{r correlation plot}
# Simple correlation matrix between effect size and all the other catchment characteristics
no3_correlation <- no3_RF %>% 
  dplyr::select(Effect_size, totdasqkm, minelevraw, maxelevsmo, minelevsmo, elevfixed, slope, Tmax9120Ws,Tmean9120Ws, Tmin9120Ws, Forest, Urban, Grassland, Wetland, AG, OmWs, BFIWs, PermWs, RunoffWs, burn_percentage, mean_dnbr, Time_Since_Fire) %>% 
  mutate(Time_Since_Fire = as.numeric(Time_Since_Fire))

# If mean DNBR values are calculated then our measurement is no bueno. Remove. 
no3_correlation <- no3_correlation %>% 
  mutate(mean_dnbr = case_when(mean_dnbr < 0 ~ NA,
                               TRUE ~ mean_dnbr))

# Calculate the correlation matrix
cor_matrix_no3 <- cor(no3_correlation, use = "complete.obs")

# Assuming cor_matrix_no3 is your correlation matrix
cor_matrix_no3 <- as.data.frame(cor_matrix_no3)

# Get the order of columns with 'Effect_size' first
column_order <- c("Effect_size", setdiff(names(cor_matrix_no3), "Effect_size"))

# Reorder columns using select
cor_matrix_no3 <- cor_matrix_no3 %>%
  dplyr::select(all_of(column_order))

# Reorder rows by converting row names to a column, ordering, then converting back
cor_matrix_no3 <- cor_matrix_no3 %>%
  rownames_to_column(var = "rowname") %>%
  arrange(factor(rowname, levels = column_order)) %>%
  column_to_rownames(var = "rowname")

# Convert back to matrix if needed
cor_matrix_no3 <- as.matrix(cor_matrix_no3)

# Print the reordered correlation matrix
print(cor_matrix_no3)

# # Filter the correlation matrix to include only values greater than 0.6
cor_matrix_filtered_no3 <- ifelse(abs(cor_matrix_no3) > 0, cor_matrix_no3, NA)


# Specify the file path and format for the output PDF file
output_file <- here("initial_plots", "08_Meta_summary_stats", "NO3_geospatial_correlation.pdf")

# Open a PDF device
pdf(file = output_file, width = 12, height = 12)

# Plot the reordered correlation matrix using corrplot
corrplot(cor_matrix_no3, method = "circle", type = "upper", order = "original", 
         tl.cex = 0.8, tl.col = "black", addgrid.col = "grey")

# Add a title to the bottom left corner
mtext("NO3 Correlation Matrix", side = 3, line = 1, adj = 0, cex = 1.2)


# Close the device to save the file
dev.off()


```

### Histograms of the distribution of data based on watershed characteristics and other exploratory plots ###

```{r filtered data}
doc_meta_random_forest <- read_csv(here("Output_for_analysis", "08_Meta_summary_stats", "doc_meta_random_forest.csv"))

no3_meta_random_forest <- read_csv(here("Output_for_analysis", "08_Meta_summary_stats", "no3_meta_random_forest.csv"))

final_dataset <- rbind(doc_meta_random_forest, no3_meta_random_forest)


```

```{r - plotting}
### TSF ####
ggplot(final_dataset, aes(x = Time_Since_Fire, fill = Time_Since_Fire)) +
  geom_bar() +
  xlab("Time Since Fire") +
  ylab("Frequency") +
  theme_bw() +
  theme(legend.position = "none")

### stream order  ####
ggplot(final_dataset, aes(x = streamorde, fill = streamorde)) +
  geom_bar() +
  xlab("Stream Order") +
  ylab("Frequency") +
  theme_bw() +
  theme(legend.position = "none")

### burn percentage ####
ggplot(final_dataset, aes(x = burn_percentage)) + 
  geom_histogram(binwidth = 10) +
  xlab("Burn Percentage") +
  ylab("Frequency") +
  theme_bw()

### Slope ####
ggplot(final_dataset, aes(x = slope)) + 
  geom_histogram(binwidth = 0.01) +
  xlab("Slope") +
  ylab("Frequency") +
  theme_bw()

### watershed area ####
ggplot(final_dataset, aes(x = totdasqkm)) + 
  geom_histogram(binwidth = 50) +
  xlab("Watershed Area") +
  ylab("Frequency") +
  theme_bw()
```


```{r ratio between burned and unburned watersheds}
library(tidyverse)
library(hrbrthemes)
watershed_area <- read_csv(here("inputs", "Studies_Summary", "Watershed_area.csv"), na = c('-9999', 'N/A')) 

burn_ratio <- watershed_area %>%
  group_by(Study_ID) %>%
  summarise(BR = sum(Watershed_Size_km[Burn_Unburn == "Burn"]) / sum(Watershed_Size_km[Burn_Unburn == "Unburn"]))

ggplot(burn_ratio, aes(x = BR)) +
  geom_histogram(binwidth = 1, fill = "#69b3a2", color = "#e9ecef", alpha = 0.9) +
  scale_x_continuous(breaks = seq(0, 40, by = 5), labels = seq(0, 40, by = 5)) +
  theme_bw() +
  ggtitle("Watershed Area (Burn) / Watershed Area (Unburn)")

ggsave("Watershed_Area_Burn_Ratio.pdf",
       path = here("initial_plots", "08_Meta_summary_stats"),
       width = 8, height = 6, units = "in")


```


# =========================================== Script Graveyard ===============================================
```{r Building Random forest dataframe}
# # Geospatial data 
# geospatial_data <- read_csv(here("Output_for_analysis", "06_Meta_geospatial_extraction_with_comids",
#                                  "random_forest_data.csv")) %>%
#   mutate(Site = str_replace_all(Site, "_", " "))
# 
# # The effect sizes of each study's watershed
# effect_size <- read_csv(here("Output_for_analysis", "04_Meta_merge_all_studies_effect_size", "Effect_Size.csv"), na = c('-9999', 'N/A')) %>% 
#   dplyr::select(Study_ID, response_var, Climate, Biome, Effect_size, Effect_size_pair, Time_Since_Fire)
# 
# # This has site and effect size pair which will merge with the effect size dataframe to get a site name with each effect size.
# study_site_data <- read_csv(here("inputs", "catchment_characteristics", "Fire_name_Lat_Long.csv"), na = c('-9999', 'N/A')) %>%
#   dplyr::select(Study_ID, Site, Fire_year, Vegetation_year_pull, Effect_size_pair, burn_percentage) %>%
#   rename(year = Fire_year) %>%
#   mutate(Site = str_replace_all(Site, "_", " "))
# 
# unique(study_site_data$Site) # Supposed to be 65 sites
# 
# no_geo_random_forest_data <- full_join(effect_size, study_site_data) %>%
#   na.omit(Effect_size) # this na.omit eliminates all the control sites that dont have an associated effect size.
# 
# unique(no_geo_random_forest_data$Site) # Supposed to be 28 sites here
# 
# # Joining our effect sizes and sites df with the geospatial dataframe to get catchment characteristics. yay.
# random_forest_data <- left_join(no_geo_random_forest_data, geospatial_data) %>% 
#   dplyr::select(!MTBS_1986Ws:MTBS_2017Ws) %>% # Pulled in from study_site_data
#   na.omit(slope) # eliminates sites that dont have associated geospatial data 
#   
# unique(random_forest_data$Site) # 24 sites
# 
# #splitting by analyte
# doc_RF <- random_forest_data %>%
#   filter(response_var == "DOC_Interp")
# 
# write_csv(doc_RF, here("Output_for_analysis", "08_Meta_summary_stats", "doc_meta_random_forest.csv"))
# 
# no3_RF <- random_forest_data %>%
#   filter(response_var == "NO3_Interp") 
# 
# write_csv(no3_RF, here("Output_for_analysis", "08_Meta_summary_stats", "no3_meta_random_forest.csv"))

# Sites that we want to pull 2 years worth for 
# Coal Creek we want 2004 and 2008 data 
# DS1 we want 2011 and 2016
# DS2 we want 2011 and 2016 
# DS3 we want 2011 and 2016 


# coal_filter_landcover_long <- landcover_long %>% 
#   filter(Site == "Coal") %>%
#   filter(Vegetation_year_pull %in% c("2004", "2008"))
# 
# DS_filter_landcover_long <- landcover_long %>% 
#   filter(Site %in% c("DS1", "DS2", "DS3")) %>%
#   filter(Vegetation_year_pull %in% c("2011", "2016"))



# 
# # Correlation matrix with significance levels (p-value)
# res2 <- rcorr(as.matrix(doc_correlation))
# res2 # the only significant p-value with effect size is watershed area
# 
# corrplot(res2$r, type="upper", order="hclust", 
#          p.mat = res2$P, sig.level = 0.05, insig = "blank")
# 
# ggpairs(doc_correlation, columns = c("Effect_size", "totdasqkm", "minelevraw", "maxelevsmo","minelevsmo", "elevfixed", "slope", "burn_percentage", "Time_Since_Fire")) +
#   theme_bw()


# 
# # Correlation matrix with significance levels (p-value)
# res2 <- rcorr(as.matrix(doc_correlation))
# res2 # the only significant p-value with effect size is watershed area
# 
# corrplot(res2$r, type="upper", order="hclust", 
#          p.mat = res2$P, sig.level = 0.05, insig = "blank")
# 
# ggpairs(doc_correlation, columns = c("Effect_size", "totdasqkm", "minelevraw", "maxelevsmo","minelevsmo", "elevfixed", "slope", "burn_percentage", "Time_Since_Fire")) +
#   theme_bw()

```










