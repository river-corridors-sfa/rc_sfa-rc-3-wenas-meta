---
title: "Crandall_paired_effect_Test"
output: html_document
date: "2023-11-14"
---
The purpose of this script is pair the watersheds in the crandall study for the gap fill to calculate effect size and compare that to the original method of calculating effect size where all the controls get aggredated down and compared to the burn 

Script Workflow:

Step 1) Load in all the individual studies from "meta_final" located within inputs in the repository.  

Step 2) Filter out studies that are Pre_Post -> we only want papers that are Control_vs_Impact

Step 3) run script that fills in concentrations for missing days in between discrete sampling days and then takes an average by month. 

# Status: in progress

# ==============================================================================
# Author: Jake Cavaiani; jake.cavaiani@pnnl.gov
# 14 November 2024
# ==============================================================================



## Temporal Normalization 
```{r Jake/Mac Load Packages and}
#for Jake/mac

rm(list=ls(all=T)) #this clears your Environment


library(tidyverse)
library(here)
library(forecastML)
library(zoo)
library(hrbrthemes)
library(viridis)

```

```{r data prepper}
meta_df <- read_csv(here("inputs", "Studies", "meta_final", "Crandall_et_al_2021.csv"),
                    na = c('-9999', 'N/A'))

# On 7/2/2024 I clipped the Benjamin Slough watershed because it was incorporating the Payson watershed. The updated watershed area is 266.787. Lets update that here so we can calculate a proper effect size. 

meta_df <- meta_df %>% 
  mutate(Area_watershed = case_when(Site == "Benjamin Slough" ~ 266.787,
                                    TRUE ~ Area_watershed))


```


```{r data prep}
control_impact <- meta_df 

# separate out the time columns 
time_format <- control_impact %>% 
  dplyr::select(Study_ID, Sampling_Date) %>% 
  mutate(Sampling_Date = as.character(Sampling_Date))

# merge this dataframe with the metadata df to make sure all the dates are the same format and there aren't any NAs
control_impact <- control_impact %>% 
  mutate(Sampling_Date = time_format$Sampling_Date)


# changing the structure of the concentrations
control_impact <- control_impact %>% 
  mutate(DOC = as.numeric(DOC),
         NO3 = as.numeric(NO3), 
         Sampling_Date = ymd(Sampling_Date))

# Changing the units to make sure everything is consistent in mg_N_L or mg_C_L
control_impact_units <- control_impact %>% 
  mutate(Area_watershed_km = case_when(Area_unit == 'ha' ~ Area_watershed * .01,
                                       Area_unit == 'km' ~ print(Area_watershed)),
         
         DOC_mg_C_L = case_when(DOC_unit == 'mg_C_L' ~ print(DOC),
                                DOC_unit == 'mg_L' ~ print(DOC),
                                DOC_unit == 'um' ~ DOC * 0.01201),
         
         NO3_mg_N_L = case_when(NO3_unit == 'um' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_NO2_NO3_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'mg_N_L' ~ print(NO3),
                                NO3_unit == 'ug_N_L' ~ NO3 * .001, 
                                NO3_unit == 'mg_L' ~ NO3 * 0.225904780),
         
         DOC_uM_C = case_when(DOC_unit == 'um' ~ print(DOC),
                              DOC_unit == 'mg_C_L' ~ DOC * 83.2639467,
                              DOC_unit == 'mg_L' ~ DOC * 83.2639467),

         NO3_uM_N = case_when(NO3_unit == 'um' ~ print(NO3),
                              NO3_unit == 'umol_L' ~ print(NO3),
                              NO3_unit == 'umol_NO2_NO3_L' ~ print(NO3),
                              NO3_unit == 'mg_L' ~ NO3 * 16.127729,
                              NO3_unit == 'mg_N_L' ~ NO3 * 3.64145101,
                              NO3_unit == 'ug_N_L' ~ NO3 * 0.22594948))




# Creating a site characteristics data frame that I will use to merge later to add in the proper data 
site_data <- control_impact_units %>% 
  dplyr::select(Study_ID, Pair, Site, latitude, longitude, Area_watershed_km, Climate, Burn_Unburn)

site_data_unique <- site_data %>% 
  group_by(Study_ID, Pair, Area_watershed_km) %>% 
  distinct(Area_watershed_km, .keep_all = TRUE)


```

```{r - Crandall }
# Okay lets do this for all the studies # 
# filling in daily time series and interpolating by study and site
control_impact_fill <-  control_impact_units %>% 
  dplyr::select("Study_ID", "latitude", "longitude", "Area_watershed_km", "Pair", "Climate", "Site", "Burn_Unburn", "Time_Since_Fire", "Sampling_Date", "DOC_mg_C_L", "NO3_mg_N_L") %>%
  group_by(Study_ID, Site) %>%
  complete(Sampling_Date = seq(min(Sampling_Date), max(Sampling_Date), by = "1 day")) %>% 
  fill(Study_ID:Time_Since_Fire) %>%
  fill(Study_ID, Site) %>%
  ungroup() %>%
  group_by(Site) %>%
  mutate(NO3_Interp = na.approx(NO3_mg_N_L, na.rm = FALSE),
         DOC_Interp = na.approx(DOC_mg_C_L, na.rm = FALSE)) %>%
  fill(Study_ID:Time_Since_Fire)
  
control_impact_fill_out <- control_impact_fill %>% 
  mutate(across(where(is.character), ~ifelse(is.na(.), "N/A", .))) %>% 
  mutate(across(where(is.logical), ~ifelse(is.na(.), "N/A", .))) %>%          
  mutate(across(where(is.numeric), ~ifelse(is.na(.), -9999, .)))
  
  
write_csv(control_impact_fill_out, here("Output_for_analysis", "01_Crandall_effect_size", "crandall_conc_fill.csv"))

```

```{r}
# This is code from my other script. I am going to try this way to see if this is what we want to get to 
# pivoting to make the responses in one column

metadata_long <- control_impact_fill %>%
  pivot_longer(
    cols = NO3_Interp:DOC_Interp,
    names_to = "response_var",
    values_to = "concentration",
    values_drop_na = TRUE
  )

metadata_long <- metadata_long %>% 
  mutate(year = year(Sampling_Date),
         month = month(Sampling_Date),
         day = day(Sampling_Date))


# aggregate by year 
yearly_mean_site <-  metadata_long %>%
  group_by(Study_ID, response_var, Pair, year) %>% 
  summarize(meanConc = mean(concentration)) %>% 
  left_join(site_data) %>% 
  group_by(Pair, year) %>%
  distinct(meanConc, .keep_all = TRUE) %>%
  mutate(TempNorm = meanConc / Area_watershed_km)
    
yearly_mean_site$TempNorm <- format(yearly_mean_site$TempNorm, scientific = F)

write_csv(yearly_mean_site, here("Output_for_analysis", "01_Crandall_effect_size", "crandall_temp_norm.csv"))

```

#### EFFECT SIZE 
```{r}
# Bringing the columns back up to match to be able to calculate
yearly_mean_site_test <- yearly_mean_site %>%
  mutate(TempNorm = as.numeric(TempNorm))  # Taking out Hauer and Spencer and Hickenbottom because they have mutliple controls and that is different than the rest of these sites

# This is doing the same thing as chunck before where we are creating a column header for each individual site but doing it for the studies that have multiple controls. 
yearly_mean_crandall <- yearly_mean_site %>%
  mutate(TempNorm = as.numeric(TempNorm))



crandall_test <- yearly_mean_crandall %>%
  group_by(Study_ID, Pair, year, Burn_Unburn, response_var, Climate) %>%
  dplyr::summarize(Xe = sum(TempNorm[Burn_Unburn == "Burn"], na.rm = TRUE),
                   Xc = sum(TempNorm[Burn_Unburn == "Unburn"], na.rm = TRUE)) # Creating a control and treatment column for the temporal normalization 

yearly_multiple_control <- crandall_test %>% 
  group_by(Study_ID, response_var, Climate, year) %>%
  summarize(Xe1 = sum(Xe[Pair == "Site_1"], na.rm = TRUE),
            Xe2 = sum(Xe[Pair == "Site_2"], na.rm = TRUE),
            Xe3 = sum(Xe[Pair == "Site_3"], na.rm = TRUE),
            Xe4 = sum(Xe[Pair == "Site_4"], na.rm = TRUE),
            Xc34 = sum(Xc[Pair == "Control_3_4"], na.rm = TRUE),
            Xc2 = sum(Xc[Pair == "Control_2"], na.rm = TRUE),
            Xc = sum(Xc[Pair == "Control_1"], na.rm = TRUE)) 


yearly_effect_multiple <- yearly_multiple_control %>% 
  group_by(Study_ID, response_var, Climate, year) %>%
  mutate(lnR1 = log(Xe1/Xc),
         lnR2 = log(Xe2/Xc2),
         lnR3 = log(Xe3/Xc34),
         lnR4 = log(Xe4/Xc34)) %>% 
  ungroup()


# pivoting to make the responses in one column
# effect size 
yearly_effect_size_combine_long <- yearly_effect_multiple %>% 
  pivot_longer(
    cols = lnR1:lnR4,
    names_to = "Effect_size_pair",
    values_to = "Effect_size",
    values_drop_na = TRUE
  ) %>% 
  filter(is.finite(Effect_size))

write_csv(yearly_effect_size_combine_long, here("Output_for_analysis", "01_Crandall_effect_size", "Crandall_effect_size.csv"))


```



####################### DOWNSELECTED APPROACH 2/19/2025 #############################
## Temporal Normalization 
```{r Jake/Mac Load Packages and}
#for Jake/mac

# rm(list=ls(all=T)) #this clears your Environment


library(tidyverse)
library(here)
library(forecastML)
library(zoo)
library(hrbrthemes)
library(viridis)

```

```{r data prepper}
meta_df <- read_csv(here("inputs", "Studies", "meta_final", "Crandall_et_al_2021.csv"),
                    na = c('-9999', 'N/A'))

# On 7/2/2024 I clipped the Benjamin Slough watershed because it was incorporating the Payson watershed. The updated watershed area is 266.787. Lets update that here so we can calculate a proper effect size. 

meta_df <- meta_df %>% 
  mutate(Area_watershed = case_when(Site == "Benjamin Slough" ~ 266.787,
                                    TRUE ~ Area_watershed))


```


```{r data prep}
control_impact <- meta_df 

# separate out the time columns 
time_format <- control_impact %>% 
  dplyr::select(Study_ID, Sampling_Date) %>% 
  mutate(Sampling_Date = as.character(Sampling_Date))

# merge this dataframe with the metadata df to make sure all the dates are the same format and there aren't any NAs
control_impact <- control_impact %>% 
  mutate(Sampling_Date = time_format$Sampling_Date)


# changing the structure of the concentrations
control_impact <- control_impact %>% 
  mutate(DOC = as.numeric(DOC),
         NO3 = as.numeric(NO3), 
         Sampling_Date = ymd(Sampling_Date))

# Changing the units to make sure everything is consistent in mg_N_L or mg_C_L
control_impact_units <- control_impact %>% 
  mutate(Area_watershed_km = case_when(Area_unit == 'ha' ~ Area_watershed * .01,
                                       Area_unit == 'km' ~ print(Area_watershed)),
         
         DOC_mg_C_L = case_when(DOC_unit == 'mg_C_L' ~ print(DOC),
                                DOC_unit == 'mg_L' ~ print(DOC),
                                DOC_unit == 'um' ~ DOC * 0.01201),
         
         NO3_mg_N_L = case_when(NO3_unit == 'um' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_NO2_NO3_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'mg_N_L' ~ print(NO3),
                                NO3_unit == 'ug_N_L' ~ NO3 * .001, 
                                NO3_unit == 'mg_L' ~ NO3 * 0.225904780),
         
         DOC_uM_C = case_when(DOC_unit == 'um' ~ print(DOC),
                              DOC_unit == 'mg_C_L' ~ DOC * 83.2639467,
                              DOC_unit == 'mg_L' ~ DOC * 83.2639467),

         NO3_uM_N = case_when(NO3_unit == 'um' ~ print(NO3),
                              NO3_unit == 'umol_L' ~ print(NO3),
                              NO3_unit == 'umol_NO2_NO3_L' ~ print(NO3),
                              NO3_unit == 'mg_L' ~ NO3 * 16.127729,
                              NO3_unit == 'mg_N_L' ~ NO3 * 3.64145101,
                              NO3_unit == 'ug_N_L' ~ NO3 * 0.22594948))




# Creating a site characteristics data frame that I will use to merge later to add in the proper data 
site_data <- control_impact_units %>% 
  dplyr::select(Study_ID, Pair, Site, latitude, longitude, Area_watershed_km, Climate, Burn_Unburn)

site_data_unique <- site_data %>% 
  group_by(Study_ID, Pair, Area_watershed_km) %>% 
  distinct(Area_watershed_km, .keep_all = TRUE)


```

```{r - Crandall }
# Okay lets do this for all the studies # 
# filling in daily time series and interpolating by study and site
control_impact_fill <-  control_impact_units %>% 
  dplyr::select("Study_ID", "latitude", "longitude", "Area_watershed_km", "Pair", "Climate", "Site", "Burn_Unburn", "Time_Since_Fire", "Sampling_Date", "DOC_mg_C_L", "NO3_mg_N_L") %>%
  group_by(Study_ID, Site) %>%
  complete(Sampling_Date = seq(min(Sampling_Date), max(Sampling_Date), by = "1 day")) %>% 
  fill(Study_ID:Time_Since_Fire) %>%
  fill(Study_ID, Site) %>%
  ungroup() %>%
  group_by(Site) %>%
  mutate(NO3_Interp = na.approx(NO3_mg_N_L, na.rm = FALSE),
         DOC_Interp = na.approx(DOC_mg_C_L, na.rm = FALSE)) %>%
  fill(Study_ID:Time_Since_Fire)
  
control_impact_fill_out <- control_impact_fill %>% 
  mutate(across(where(is.character), ~ifelse(is.na(.), "N/A", .))) %>% 
  mutate(across(where(is.logical), ~ifelse(is.na(.), "N/A", .))) %>%          
  mutate(across(where(is.numeric), ~ifelse(is.na(.), -9999, .)))
  
  
write_csv(control_impact_fill_out, here("Output_for_analysis", "01_Crandall_effect_size", "crandall_conc_fill.csv"))

```

```{r}
# This is code from my other script. I am going to try this way to see if this is what we want to get to 
# pivoting to make the responses in one column

metadata_long <- control_impact_fill %>%
  pivot_longer(
    cols = NO3_Interp:DOC_Interp,
    names_to = "response_var",
    values_to = "concentration",
    values_drop_na = TRUE
  )

metadata_long <- metadata_long %>% 
  mutate(year = year(Sampling_Date),
         month = month(Sampling_Date),
         day = day(Sampling_Date))


# aggregate by year 
yearly_mean_site <-  metadata_long %>%
  group_by(Study_ID, response_var, Pair, year) %>% 
  summarize(meanConc = mean(concentration)) %>% 
  left_join(site_data) %>% 
  group_by(Pair, year) %>%
  distinct(meanConc, .keep_all = TRUE) %>%
  mutate(TempNorm = meanConc / Area_watershed_km)
    
yearly_mean_site$TempNorm <- format(yearly_mean_site$TempNorm, scientific = F)

write_csv(yearly_mean_site, here("Output_for_analysis", "01_Crandall_effect_size", "crandall_temp_norm.csv"))

```

#### EFFECT SIZE 
```{r}
# Bringing the columns back up to match to be able to calculate
yearly_mean_site_test <- yearly_mean_site %>%
  mutate(TempNorm = as.numeric(TempNorm))  # Taking out Hauer and Spencer and Hickenbottom because they have mutliple controls and that is different than the rest of these sites

# This is doing the same thing as chunck before where we are creating a column header for each individual site but doing it for the studies that have multiple controls. 
yearly_mean_crandall <- yearly_mean_site %>%
  mutate(TempNorm = as.numeric(TempNorm))



crandall_test <- yearly_mean_crandall %>%
  group_by(Study_ID, Pair, year, Burn_Unburn, response_var, Climate) %>%
  dplyr::summarize(Xe = sum(TempNorm[Burn_Unburn == "Burn"], na.rm = TRUE),
                   Xc = sum(TempNorm[Burn_Unburn == "Unburn"], na.rm = TRUE)) # Creating a control and treatment column for the temporal normalization 

yearly_multiple_control <- crandall_test %>% 
  group_by(Study_ID, response_var, Climate, year) %>%
  summarize(Xe1 = sum(Xe[Pair == "Site_1"], na.rm = TRUE),
            Xe2 = sum(Xe[Pair == "Site_2"], na.rm = TRUE),
            Xe3 = sum(Xe[Pair == "Site_3"], na.rm = TRUE),
            Xe4 = sum(Xe[Pair == "Site_4"], na.rm = TRUE),
            Xc34 = sum(Xc[Pair == "Control_3_4"], na.rm = TRUE),
            Xc2 = sum(Xc[Pair == "Control_2"], na.rm = TRUE),
            Xc = sum(Xc[Pair == "Control_1"], na.rm = TRUE)) 


yearly_effect_multiple <- yearly_multiple_control %>% 
  group_by(Study_ID, response_var, Climate, year) %>%
  mutate(lnR1 = log(Xe1/Xc),
         lnR2 = log(Xe2/Xc2),
         lnR3 = log(Xe3/Xc34),
         lnR4 = log(Xe4/Xc34)) %>% 
  ungroup()


# pivoting to make the responses in one column
# effect size 
yearly_effect_size_combine_long <- yearly_effect_multiple %>% 
  pivot_longer(
    cols = lnR1:lnR4,
    names_to = "Effect_size_pair",
    values_to = "Effect_size",
    values_drop_na = TRUE
  ) %>% 
  filter(is.finite(Effect_size))

write_csv(yearly_effect_size_combine_long, here("Output_for_analysis", "01_Crandall_effect_size", "Crandall_effect_size.csv"))


```




