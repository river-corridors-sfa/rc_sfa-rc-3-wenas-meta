---
title: "All_Studies_Temporal_Normalization"
output: html_document
date: "2023-10-19"
editor_options: 
  chunk_output_type: console
---

The purpose of this script is to read in the studies from the final meta data list and to normalize daily stream concentrations by time. 

Script Workflow:

Step 1) Load in all the individual studies from "meta_final" located within inputs in the repository.  

Step 2) Filter out studies that are Pre_Post -> we only want papers that are Control_vs_Impact

Step 3) run script that fills in concentrations for missing days in between discrete sampling days and then takes an average by month. 


# Status: in progress

# ==============================================================================
# Author: Jake Cavaiani 
# 18 October 2023
# ==============================================================================

## Load packages and set working directory
```{r Jake/Mac}
#for Jake/mac

rm(list=ls(all=T)) #this clears your Environment


library(tidyverse)
library(here)
library(forecastML)
library(zoo)
library(hrbrthemes)
library(viridis)

```

```{r data prepper}
studies_file_list <- list.files(path = "inputs/Studies/meta_final/", 
                                  recursive=F, 
                                  pattern=".csv", 
                                  full.names=TRUE)

storm_list_beta<-do.call("list", lapply(studies_file_list, 
                                        read.csv, 
                                        stringsAsFactors=FALSE, 
                                        header=T))

meta_df <-do.call("rbind", lapply(studies_file_list, 
                                     read.csv, 
                                     check.names = FALSE,
                                     stringsAsFactors=FALSE, 
                                     header=T, blank.lines.skip = TRUE, fill=TRUE)) 

unitconv=read.csv("inputs/UnitConversion.csv",stringsAsFactors = FALSE)



```


```{r data prep}
options(max.print = 25)
# Filtering by only the control vs.impact studies because the pre_post are going to go through a different calculation 
# Also filtering out the studies where I don't have exact sampling days 
control_impact <- meta_df %>% 
  filter(`Design (Control/Burn; Pre/Post)` == "Control_Reference_vs_Impact",
         Study_ID != "Caldwell et al. 2020",
         Study_ID != "Stephan et al. 2015",
         Study_ID != "Writer & Murphy, 2012", 
         Mean_Median_or_IndividualSample == "Individual",
         as.numeric(Time_Since_Fire) < 5)

# separate out the time columns 
time_format <- control_impact %>% 
  select(Study_ID, Sampling_Date) 

# Make them all the same format and then merge back together in one column
time_format <- time_format %>% 
  mutate(DateTime = mdy(Sampling_Date),
         DateTime2 = mdy_hm(Sampling_Date), 
         DateTime2 = as.Date(DateTime2)) %>% 
  select(-Sampling_Date) %>% 
  mutate(Sampling_Date = coalesce(DateTime, DateTime2),
         Sampling_Date = as.character(Sampling_Date)) %>% 
  select(-DateTime, -DateTime2) 

# merge this dataframe with the metadata df to make sure all the dates are the same format and there aren't any NAs
control_impact <- control_impact %>% 
  mutate(Sampling_Date = time_format$Sampling_Date)


# changing the structure of the concentrations
control_impact <- control_impact %>% 
  mutate(DOC = as.numeric(DOC),
         NO3 = as.numeric(NO3), 
         Sampling_Date = ymd(Sampling_Date))

# Changing the units to make sure everything is consistent in mg_N_L or mg_C_L
control_impact_units <- control_impact %>% 
  mutate(Area_watershed_km = case_when(Area_unit == 'ha' ~ Area_watershed * .01,
                                       Area_unit == 'km' ~ print(Area_watershed)),
         
         DOC_mg_C_L = case_when(DOC_unit == 'mg_C_L' ~ print(DOC),
                                DOC_unit == 'mg_L' ~ print(DOC),
                                DOC_unit == 'um' ~ DOC * 0.01201),
         
         NO3_mg_N_L = case_when(NO3_unit == 'um' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_NO2_NO3_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'mg_N_L' ~ print(NO3),
                                NO3_unit == 'ug_N_L' ~ NO3 * .001, 
                                NO3_unit == 'mg_L' ~ NO3 * 0.225904780),
         
         DOC_uM_C = case_when(DOC_unit == 'um' ~ print(DOC),
                              DOC_unit == 'mg_C_L' ~ DOC * 83.2639467,
                              DOC_unit == 'mg_L' ~ DOC * 83.2639467),

         NO3_uM_N = case_when(NO3_unit == 'um' ~ print(NO3),
                              NO3_unit == 'umol_L' ~ print(NO3),
                              NO3_unit == 'umol_NO2_NO3_L' ~ print(NO3),
                              NO3_unit == 'mg_L' ~ NO3 * 16.127729,
                              NO3_unit == 'mg_N_L' ~ NO3 * 3.64145101,
                              NO3_unit == 'ug_N_L' ~ NO3 * 0.22594948))




# Creating a site characteristics data frame that I will use to merge later to add in the proper data 
site_data <- control_impact_units %>% 
  select(Study_ID, Pair, Latitude, Longitude, Area_watershed_km, Climate, Burn_Unburn,
         Time_Since_Fire)

# Selection process from total metadata
table(meta_df$Study_ID) # 36 studies 

pre_post <- meta_df %>% 
  filter(`Design (Control/Burn; Pre/Post)` == "Pre_Post") 
table(pre_post$Study_ID) # 5 studies that are pre_post so that brings us to 31 total 

control_pre_post <- meta_df %>% 
  filter(`Design (Control/Burn; Pre/Post)` == "Control_Reference_vs_Impact_Pre_Post") 
table(control_pre_post$Study_ID) # 2 additional studies have both so those are also removed
# now at 29 total 

# Caldwell, Stephan, and writer and murphy dont have exact dates so that will elimate those 3 studies. 
# now at 25 total

# Abbott and goodale had fires greater than 5 years ago

# now at 23

mean <- meta_df %>% 
  filter(Mean_Median_or_IndividualSample == "Mean")
table(mean$Study_ID) # 6 additional studies that report means 

# now at 17

# Does this match?
table(control_impact$Study_ID)
# 17 
# YES IT DOES


```

```{r - all the studies}
# Okay lets do this for all the studies # 
# filling in daily time series and interpolating by study and site
control_impact_fill <-  control_impact_units %>% 
  select("Study_ID", "Latitude", "Longitude", "Area_watershed_km", "Pair", "Climate", "Site", "Burn_Unburn", "Time_Since_Fire", "Sampling_Date", "DOC_uM_C", "NO3_uM_N") %>%
  group_by(Study_ID, Site) %>%
  complete(Sampling_Date = seq(min(Sampling_Date), max(Sampling_Date), by = "1 day")) %>% 
  fill(Study_ID:Time_Since_Fire) %>%
  fill(Study_ID, Site) %>%
  ungroup() %>%
  group_by(Site) %>%
  mutate(NO3_Interp = na.approx(NO3_uM_N, na.rm = FALSE),
         DOC_Interp = na.approx(DOC_uM_C, na.rm = FALSE)) %>%
  fill(Study_ID:Time_Since_Fire)
  

```


```{r}
# This is code from my other script. I am going to try this way to see if this is what we want to get to 
# pivoting to make the responses in one column
metadata_long <- control_impact_fill %>%
  pivot_longer(
    cols = NO3_Interp:DOC_Interp,
    names_to = "response_var",
    values_to = "concentration",
    values_drop_na = TRUE
  )

metadata_long <- metadata_long %>% 
  mutate(year = year(Sampling_Date),
         month = month(Sampling_Date),
         day = day(Sampling_Date))

monthly_mean_site <-  metadata_long %>%
  group_by(Study_ID, response_var, Pair, year, month) %>% 
  summarize(meanConc = mean(concentration)) %>% 
  left_join(site_data) %>% 
  group_by(month) %>% 
  distinct(meanConc, .keep_all = TRUE) %>% 
  mutate(TempNorm = meanConc * Area_watershed_km)

# Check to make sure the output is accurate
abbott <- monthly_mean_site %>% 
  filter(Study_ID == "Abbott et al. 2021")

burd <- monthly_mean_site %>% 
  filter(Study_ID == "Burd et al 2018")

burd_site <- site_data %>% 
  filter(Study_ID == "Burd et al 2018")

crandall <- monthly_mean_site %>% 
  filter(Study_ID == "Crandall et al. 2021")

# aggregate by year 
yearly_mean_site <-  metadata_long %>%
  group_by(Study_ID, response_var, Pair, year) %>% 
  summarize(meanConc = mean(concentration)) %>% 
  left_join(site_data) %>% 
  group_by(year) %>% 
  distinct(meanConc, .keep_all = TRUE) %>% 
  mutate(TempNorm = meanConc / Area_watershed_km)

burd <- yearly_mean_site %>% 
  filter(Study_ID == "Burd et al 2018")

coombs <- yearly_mean_site %>% 
  filter(Study_ID == "Coombs & Melack, 2013")




# Table of each climate guild #
table(monthly_mean_site$Climate)

Csa <- monthly_mean_site %>% 
  filter(Climate == "Csa")
table(Csa$Study_ID) # Uzun 

Dfb <- monthly_mean_site %>% 
  filter(Climate == "Dfb")
table(Dfb$Study_ID) # Wagner et al. 2015, Writer et al. 2014

table(monthly_mean_site$Study_ID)

table(yearly_mean_site$Study_ID)
# Calculating the standard deviation for all of the TempNorms
climate.summary <- monthly_mean_site %>%
  group_by(Climate, response_var, Burn_Unburn) %>%
  summarise(sd = sd(TempNorm, na.rm = TRUE),
            TempNorm = mean(TempNorm, na.rm = TRUE))

# Calculating the standard deviation for all of the TempNorms for yearly aggregate
climate.summary.year <- yearly_mean_site %>%
  group_by(Climate, response_var, Burn_Unburn) %>%
  summarise(sd = sd(TempNorm, na.rm = TRUE),
            TempNorm = mean(TempNorm, na.rm = TRUE))




```


```{r - plots}
# Plotting #
vn = expression(paste(""*N*O[3]^"-"))
# um = expression(paste(""*N*O[3]^"-"))

ggplot(monthly_mean_site, aes(x = log(TempNorm), y = Climate, color = response_var, shape = Burn_Unburn),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0.09, size = 3) +
  geom_pointrange(aes(xmin = log(TempNorm - sd), xmax = log(TempNorm + sd), 
                      color = response_var),
                      position = position_dodge(width = -0.5), size = 1.5, 
                      data = climate.summary) +
  geom_vline(xintercept = 0, linewidth = 0.5, color = "red") +
  ylab("Koppen (Climate Classification)") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = "Analyte"),
                     labels = c('DOC', vn)) +
  theme(axis.text.x = element_text(size = 30, angle = -90),
        axis.text.y = element_text(size = 30),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 34),
        legend.text = element_text(size = 30),
        legend.position = c(0.9, 0.65)) +
  theme_bw()

ggplot(monthly_mean_site, aes(x = TempNorm, y = Climate, color = response_var, shape = Burn_Unburn),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0.09, size = 3) +
  geom_pointrange(aes(xmin = TempNorm - sd, xmax = TempNorm + sd, 
                      color = response_var),
                      position = position_dodge(width = -0.5), size = 1.5, 
                      data = climate.summary) +
  geom_vline(xintercept = 0, linewidth = 0.5, color = "red") +
  ylab("Koppen (Climate Classification)") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = "Analyte"),
                     labels = c('DOC', vn)) +
  theme(axis.text.x = element_text(size = 30, angle = -90),
        axis.text.y = element_text(size = 30),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 34),
        legend.text = element_text(size = 30),
        legend.position = c(0.9, 0.65)) +
  theme_bw()

# Yearly aggregate
ggplot(yearly_mean_site, aes(x = TempNorm, y = Climate, color = response_var, shape = Burn_Unburn),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0.09, size = 3) +
  geom_pointrange(aes(xmin = TempNorm - sd, xmax = TempNorm + sd, 
                      color = response_var),
                      position = position_dodge(width = -0.5), size = 1.5, 
                      data = climate.summary.year) +
  xlim(-5, 30) +
  geom_vline(xintercept = 0, linewidth = 0.5, color = "red") +
  ylab("Koppen (Climate Classification)") +
  xlab("uM km-2 yr-1") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = "Analyte"),
                     labels = c('DOC', vn)) +
  theme(axis.text.x = element_text(size = 30, angle = -90),
        axis.text.y = element_text(size = 30),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 34),
        legend.text = element_text(size = 30),
        legend.position = c(0.9, 0.65)) +
  theme_bw()



```


```{r - code graveyard}
# # Fill gaps
# control_impact_gap_fill <- control_impact %>% 
#   group_by(Study_ID) %>% 
#   complete(Sampling_Date = seq.Date(min(Sampling_Date), max(Sampling_Date), by = "day")) %>% 
#   mutate(NO3 = na.approx(NO3))  
#   
# 
# df <- tibble(
#   Group = c("A", "A", "A", "B", "B"),
#   Date = as.Date(c("2023-10-01", "2023-10-02", "2023-10-04", "2023-10-01", "2023-10-03")),
#   Value = c(10, 15, 20, 5, 10)
# )
# 
# df_filled <- df %>%
#   group_by(Group) %>%
#   complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
#   fill(Value, .direction = "up")


# control_impact_summary <- control_impact_fill %>% 
#   group_by(Study_ID, Site, year, month) %>% 
#   summarise(XeNO3 = mean(NO3_Interp[Burn_Unburn == "Burn"], na.rm = TRUE),
#             XcNO3 = mean(NO3_Interp[Burn_Unburn == "Unburn"], na.rm = TRUE),
#             XeDOC = mean(DOC_Interp[Burn_Unburn == "Burn"], na.rm = TRUE),
#             XcDOC = mean(DOC_Interp[Burn_Unburn == "Unburn"], na.rm = TRUE))
# 
# 
# 
#   
# gerla_month_mean_burn <- gerla_burn_fill %>%
#   group_by(Climate, year, month) %>%
#   summarize(Xe = mean(NO3[Burn_Unburn == "Burn"], na.rm = TRUE),
#             TimeNorm_Burn = Xe * 6423/100)


# MeanConcTest <- metadata_long %>%
#   group_by(Study_ID, Pair, response_var) %>%
#   summarize(Ne = sum(Burn_Unburn == "Burn"),
#                    Nc = sum(Burn_Unburn == "Unburn"),
#                    Xe = mean(mean_concentration[Burn_Unburn == "Burn"], na.rm = TRUE),
#                    Xc = mean(mean_concentration[Burn_Unburn == "Unburn"], na.rm = TRUE))
# 
# # This is calculating the mean for EACH site within a study_ID # 
#     # Any study_ID that doesn't have up to 10 sites will have a 0 in the respective Xen column 
# analyte_table_means
# 
# 
# analyte_table_means <- MeanConcTest %>%
#   select(-Pair) %>%
#   group_by(Study_ID, response_var) %>%
#   summarize(Ne = sum(Ne),
#             Nc = sum(Nc),
#             Xe1 = sum(Xe[Pair == "Site_1"], na.rm = TRUE),
#             Xe2 = sum(Xe[Pair == "Site_2"], na.rm = TRUE),
#             Xe3 = sum(Xe[Pair == "Site_3"], na.rm = TRUE),
#             Xe4 = sum(Xe[Pair == "Site_4"], na.rm = TRUE),
#             Xe5 = sum(Xe[Pair == "Site_5"], na.rm = TRUE),
#             Xe6 = sum(Xe[Pair == "Site_6"], na.rm = TRUE),
#             Xe7 = sum(Xe[Pair == "Site_7"], na.rm = TRUE),
#             Xe8 = sum(Xe[Pair == "Site_8"], na.rm = TRUE),
#             Xe9 = sum(Xe[Pair == "Site_9"], na.rm = TRUE),
#             Xe10 = sum(Xe[Pair == "Site_10"], na.rm = TRUE),
#             Xe11 = sum(Xe[Pair == "Site_11"], na.rm = TRUE),
#             Xe12 = sum(Xe[Pair == "Site_12"], na.rm = TRUE),
#             Xe13 = sum(Xe[Pair == "Site_13"], na.rm = TRUE),
#             Xe14 = sum(Xe[Pair == "Site_14"], na.rm = TRUE),
#             Xe15 = sum(Xe[Pair == "Site_15"], na.rm = TRUE),
#             Xe16 = sum(Xe[Pair == "Site_16"], na.rm = TRUE),
#             Xe17 = sum(Xe[Pair == "Site_17"], na.rm = TRUE),
#             Xe18 = sum(Xe[Pair == "Site_18"], na.rm = TRUE),
#             Xe19 = sum(Xe[Pair == "Site_19"], na.rm = TRUE),
#             Xe20 = sum(Xe[Pair == "Site_20"], na.rm = TRUE),
#             Xe21 = sum(Xe[Pair == "Site_21"], na.rm = TRUE),
#             Xe22 = sum(Xe[Pair == "Site_22"], na.rm = TRUE),
#             Xc = sum(Xc[Pair == "Control"], na.rm = TRUE),
#             Xc1 = sum(Xc[Pair == "Control_1"], na.rm = TRUE),
#             Xc2 = sum(Xc[Pair == "Control_2"], na.rm = TRUE),
#             Xc345 = sum(Xc[Pair == "Control_3_4_5"], na.rm = TRUE),
#             ) 
# 
# # making all the cells with 0 due to no sites beyond what the study had NAs to visualize the dataframe a little better. 
# analyte_table_means[analyte_table_means == 0] <- NA
# 
# # check here to make sure this is calculating what you want it to calculate. 
# #lets filter out Coombs and Melack here to check 
# coombs_fill_check <- control_impact_fill %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# burd_fill_check <- analyte_table_means %>% 
#   filter(Study_ID == "Burd et al 2018")
# 
# write_csv(coombs_fill_check, here("Output_for_analysis", "coombs_fill_check.csv"))
# 
# coombs_long_check <- metadata_long %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# coombs_mean_check <- MeanConcTest %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# coombs_summary_check <- analyte_table_means %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# 
# coombs_check <- metadata_long %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")


# Abbott test #
# abbott <- control_impact %>% 
#   filter(Study_ID == "Abbott et al. 2021")
# 
# abbott_time_fill <- abbott %>% 
#   complete(Sampling_Date = seq.Date(min(Sampling_Date), max(Sampling_Date), by = "day")) %>% 
#   select("Sampling_Date", "Site", "DOC", "NO3") 
# 
# ggplot(abbott_time_fill, aes(x = Sampling_Date, y = NO3)) +
#   geom_point() +
#   facet_wrap(~Site, scales = "free") +
#   theme_bw()
# 
# 
# abbott_time_fill_filter <- abbott_time_fill %>% 
#   filter(Site %in% c("BS.11", "BS.12"))
# 
# abbott_time_fill_filter <- abbott_time_fill_filter %>% 
#   fill_gaps(date_col = 1, frequency = '1 day',
#             groups = 'Site') %>% 
#   mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
#          DOC_Interp = na.approx(DOC, na.rm = FALSE))
# 
# 
#   
# abbott_solute_fill <-  abbott_time_fill %>%
#   fill_gaps(date_col = 1, frequency = '1 day',
#             groups = 'Site') %>% 
#   mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
#          DOC_Interp = na.approx(DOC, na.rm = FALSE))
# 
# ggplot(abbott_solute_fill, aes(x = Sampling_Date, y = NO3_Interp)) +
#   geom_point() +
#   facet_wrap(~Site, scales = "free") +
#   theme_bw()


# lets try to do this for 2 studies # 
# multiple <- control_impact %>% 
#   filter(Study_ID %in% c("Abbott et al. 2021", "Burd et al 2018"))
# 
# multiple_time_fill <- multiple %>% 
#   select("Study_ID", "Sampling_Date", "Site", "DOC", "NO3") 
# 
# multiple_time_fill <- multiple_time_fill %>%
#   group_by(Study_ID) %>% 
#   fill_gaps(date_col = 2, frequency = '1 day',
#             groups = 'Site') %>% 
#   mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
#          DOC_Interp = na.approx(DOC, na.rm = FALSE))


# # Hauer test #
# hauer <- control_impact %>% 
#   filter(Study_ID == "Hauer & Spencer 1998")
# 
# hauer_time_fill <- hauer %>% 
#   select("Sampling_Date", "Site", "NO3") 
# 
# hauer_time_fill_filter <- hauer_time_fill %>% 
#   group_by(Site) %>% 
#   complete(Sampling_Date = seq(min(Sampling_Date), max(Sampling_Date), by = "1 day"))
# 
# 
# 
# ggplot(coombs_time_fill_filter, aes(x = Sampling_Date, y = NO3_Interp)) +
#   geom_point() +
#   facet_wrap(~Site, scales = "free") +
#   theme_bw()
```












