---
title: "All_Studies_Temporal_Normalization"
output: html_document
date: "2023-10-19"
editor_options: 
  chunk_output_type: console
---

The purpose of this script is to read in the studies from the final meta data list and to normalize daily stream concentrations by time. 

Script Workflow:

Step 1) Load in all the individual studies from "meta_final" located within inputs in the repository.  

Step 2) Filter out studies that are Pre_Post -> we only want papers that are Control_vs_Impact

Step 3) run script that fills in concentrations for missing days in between discrete sampling days and then takes an average by month. 


# Status: in progress

# ==============================================================================
# Author: Jake Cavaiani 
# 18 October 2023
# ==============================================================================

## Temporal Normalization 
```{r Jake/Mac Load Packages and}
#for Jake/mac

rm(list=ls(all=T)) #this clears your Environment


library(tidyverse)
library(here)
library(forecastML)
library(zoo)
library(hrbrthemes)
library(viridis)

```

```{r data prepper}
studies_file_list <- list.files(path = "inputs/Studies/meta_final/", 
                                  recursive=F, 
                                  pattern=".csv", 
                                  full.names=TRUE)

storm_list_beta<-do.call("list", lapply(studies_file_list, 
                                        read.csv, 
                                        stringsAsFactors=FALSE, 
                                        header=T))

meta_df <-do.call("rbind", lapply(studies_file_list, 
                                     read.csv, 
                                     check.names = FALSE,
                                     stringsAsFactors=FALSE, 
                                     header=T, blank.lines.skip = TRUE, fill=TRUE)) 

unitconv=read.csv("inputs/UnitConversion.csv",stringsAsFactors = FALSE)



```


```{r data prep}
options(max.print = 40)
# Filtering by only the control vs.impact studies because the pre_post are going to go through a different calculation 
# Also filtering out the studies where I don't have exact sampling days 
# Crandall is filtered out because the burned and reference sites are not paired so I do that is a separate script and will read that csv in later to merge to this dataframe to get the more accurate effect size calculations 

control_impact <- meta_df %>% 
  filter(`Design (Control/Burn; Pre/Post)` == "Control_Reference_vs_Impact",
         Study_ID != "Caldwell et al. 2020",
         Study_ID != "Stephan et al. 2015",
         Study_ID != "Writer & Murphy, 2012", 
         Study_ID != "Crandall et al. 2021",
         Mean_Median_or_IndividualSample == "Individual",
         as.numeric(Time_Since_Fire) < 6)

# separate out the time columns 
time_format <- control_impact %>% 
  select(Study_ID, Sampling_Date) 

# Make them all the same format and then merge back together in one column
time_format <- time_format %>% 
  mutate(DateTime = mdy(Sampling_Date),
         DateTime2 = mdy_hm(Sampling_Date), 
         DateTime2 = as.Date(DateTime2)) %>% 
  select(-Sampling_Date) %>% 
  mutate(Sampling_Date = coalesce(DateTime, DateTime2),
         Sampling_Date = as.character(Sampling_Date)) %>% 
  select(-DateTime, -DateTime2) 

# merge this dataframe with the metadata df to make sure all the dates are the same format and there aren't any NAs
control_impact <- control_impact %>% 
  mutate(Sampling_Date = time_format$Sampling_Date)


# changing the structure of the concentrations
control_impact <- control_impact %>% 
  mutate(DOC = as.numeric(DOC),
         NO3 = as.numeric(NO3), 
         Sampling_Date = ymd(Sampling_Date))

# Changing the units to make sure everything is consistent in mg_N_L or mg_C_L
control_impact_units <- control_impact %>% 
  mutate(Area_watershed_km = case_when(Area_unit == 'ha' ~ Area_watershed * .01,
                                       Area_unit == 'km' ~ print(Area_watershed)),
         
         DOC_mg_C_L = case_when(DOC_unit == 'mg_C_L' ~ print(DOC),
                                DOC_unit == 'mg_L' ~ print(DOC),
                                DOC_unit == 'um' ~ DOC * 0.01201),
         
         NO3_mg_N_L = case_when(NO3_unit == 'um' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_NO2_NO3_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'mg_N_L' ~ print(NO3),
                                NO3_unit == 'ug_N_L' ~ NO3 * .001, 
                                NO3_unit == 'mg_L' ~ NO3 * 0.225904780),
         
         DOC_uM_C = case_when(DOC_unit == 'um' ~ print(DOC),
                              DOC_unit == 'mg_C_L' ~ DOC * 83.2639467,
                              DOC_unit == 'mg_L' ~ DOC * 83.2639467),

         NO3_uM_N = case_when(NO3_unit == 'um' ~ print(NO3),
                              NO3_unit == 'umol_L' ~ print(NO3),
                              NO3_unit == 'umol_NO2_NO3_L' ~ print(NO3),
                              NO3_unit == 'mg_L' ~ NO3 * 16.127729,
                              NO3_unit == 'mg_N_L' ~ NO3 * 3.64145101,
                              NO3_unit == 'ug_N_L' ~ NO3 * 0.22594948))




# Creating a site characteristics data frame that I will use to merge later to add in the proper data 
site_data <- control_impact_units %>% 
  select(Study_ID, Pair, Latitude, Longitude, Area_watershed_km, Climate, Burn_Unburn)

site_data_unique <- site_data %>% 
  group_by(Study_ID, Pair, Area_watershed_km) %>% 
  distinct(Area_watershed_km, .keep_all = TRUE)

# Selection process from total metadata
table(meta_df$Study_ID) # 36 studies 

pre_post <- meta_df %>% 
  filter(`Design (Control/Burn; Pre/Post)` == "Pre_Post") 
table(pre_post$Study_ID) # 5 studies that are pre_post so that brings us to 31 total 

control_pre_post <- meta_df %>% 
  filter(`Design (Control/Burn; Pre/Post)` == "Control_Reference_vs_Impact_Pre_Post") 
table(control_pre_post$Study_ID) # 2 additional studies have both so those are also removed
# now at 29 total 

# Caldwell, Stephan, and writer and murphy dont have exact dates so that will elimate those 3 studies. 
# now at 25 total

# Abbott and goodale had fires greater than 5 years ago

# now at 23

mean <- meta_df %>% 
  filter(Mean_Median_or_IndividualSample == "Mean")
table(mean$Study_ID) # 6 additional studies that report means 

# now at 17

# Does this match?
table(control_impact$Study_ID)
# 17 
# YES IT DOES


```

```{r - all the studies}
# Okay lets do this for all the studies # 
# filling in daily time series and interpolating by study and site
control_impact_fill <-  control_impact_units %>% 
  select("Study_ID", "Latitude", "Longitude", "Area_watershed_km", "Pair", "Climate", "Site", "Burn_Unburn", "Sampling_Date", "DOC_mg_C_L", "NO3_mg_N_L") %>%
  group_by(Study_ID, Site) %>%
  complete(Sampling_Date = seq(min(Sampling_Date), max(Sampling_Date), by = "1 day")) %>% 
  fill(Study_ID:Burn_Unburn) %>%
  fill(Study_ID, Site) %>%
  ungroup() %>%
  group_by(Site) %>%
  mutate(NO3_Interp = na.approx(NO3_mg_N_L, na.rm = FALSE),
         DOC_Interp = na.approx(DOC_mg_C_L, na.rm = FALSE)) %>%
  fill(Study_ID:Burn_Unburn)


control_impact_fill <- control_impact_fill %>% 
  mutate(year = year(Sampling_Date),
         month = month(Sampling_Date),
         day = day(Sampling_Date))
  

```

```{r - plot summary stats}
# unique(control_impact_fill$Time_Since_Fire)
# control_impact_fill$Time_Since_Fire <- factor(control_impact_fill$Time_Since_Fire, levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "13", "15", "80", "90", "110", "Pre", "Post"))
# 
# ggplot(control_impact_fill, aes(x = Time_Since_Fire, fill = Time_Since_Fire)) +
#   geom_bar() +
#   xlab("Time Since Fire") +
#   ylab("Frequency") +
#   theme_bw() +
#   theme(legend.position = "none")


```

```{r}
# This is code from my other script. I am going to try this way to see if this is what we want to get to 
# pivoting to make the responses in one column
metadata_long <- control_impact_fill %>%
  pivot_longer(
    cols = NO3_Interp:DOC_Interp,
    names_to = "response_var",
    values_to = "concentration",
    values_drop_na = TRUE
  )


# aggregate by year 
yearly_mean_site <-  metadata_long %>%
  group_by(Study_ID, response_var, Pair, year) %>% 
  summarize(meanConc = mean(concentration)) %>% 
  left_join(site_data) %>% 
  group_by(year) %>% 
  distinct(meanConc, .keep_all = TRUE) %>% 
  mutate(TempNorm = meanConc / Area_watershed_km) 

yearly_mean_site$TempNorm <- format(yearly_mean_site$TempNorm, scientific = F)

# Calculating the standard deviation for all of the TempNorms for yearly aggregate
climate.summary.year <- yearly_mean_site %>%
  group_by(Climate, response_var, Burn_Unburn) %>%
  summarise(sd = sd(TempNorm, na.rm = TRUE),
            TempNorm = mean(TempNorm, na.rm = TRUE))




```


```{r - plots}
# Plotting #
vn = expression(paste(""*N*O[3]^"-"))
# um = expression(paste(""*N*O[3]^"-"))

give.n <- function(x){
  return(c(y = median(x)*1.5, label = length(x))) 
  # experiment with the multiplier to find the perfect position
}


# Yearly aggregate
ggplot(yearly_mean_site, aes(x = as.numeric(TempNorm), y = Climate, color = response_var, shape = Burn_Unburn),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0.5, size = 3) +
  geom_pointrange(aes(xmin = TempNorm - sd, xmax = TempNorm + sd, 
                      color = response_var),
                      position = position_dodge(width = -0.5), size = 1.5, 
                      data = climate.summary.year) +
  geom_vline(xintercept = 0, linewidth = 0.5, color = "red") +
  xlim(-5,30) +
  ylab("Koppen (Climate Classification)") +
  xlab("uM km-2 yr-1") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = "Analyte"),
                     labels = c('DOC', vn)) +
  stat_summary(fun.data = give.n, geom = "text", fun.y = median,
                  position = position_nudge(x = 20)) +
  theme(axis.text.x = element_text(size = 30, angle = -90),
        axis.text.y = element_text(size = 30),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 34),
        legend.text = element_text(size = 30),
        legend.position = c(0.9, 0.65)) +
  theme_bw()



```

#### EFFECT SIZE 
```{r}
# Bringing the columns back up to match to be able to calculate
yearly_mean_site_test <- yearly_mean_site %>%
  filter(Study_ID != "Hauer & Spencer 1998", Study_ID != "Hickenbottom et al. 2023") %>%
  mutate(TempNorm = as.numeric(TempNorm))  # Taking out Hauer and Spencer and Hickenbottom because they have mutliple controls and that is different than the rest of these sites

yearly_mean_site_test_2 <- yearly_mean_site_test %>%
  group_by(Study_ID, Pair, year, Burn_Unburn, response_var, Climate) %>%
  dplyr::summarize(Xe = sum(TempNorm[Burn_Unburn == "Burn"], na.rm = TRUE),
                   Xc = sum(TempNorm[Burn_Unburn == "Unburn"], na.rm = TRUE)) # Creating a control and treatment column for the temporal normalization 

yearly_mean_site_test_3 <- yearly_mean_site_test_2 %>%
  select(-Pair) %>%
  group_by(Study_ID, response_var, Climate, year) %>%
  summarize(Xe1 = sum(Xe[Pair == "Site_1"], na.rm = TRUE),
            Xe2 = sum(Xe[Pair == "Site_2"], na.rm = TRUE),
            Xe3 = sum(Xe[Pair == "Site_3"], na.rm = TRUE),
            Xe4 = sum(Xe[Pair == "Site_4"], na.rm = TRUE),
            Xe5 = sum(Xe[Pair == "Site_5"], na.rm = TRUE),
            Xc = sum(Xc[Pair == "Control"], na.rm = TRUE)) # Creating an individual column for each site and merging them into one row. 


# yearly_mean_site_test_2$TempNorm <- format(yearly_mean_site_test_2$TempNorm, scientific = F)

# Calculating effect size: ln((MeanTreatment/MeanControl)) for all the sites with only one control 
yearly_effect_size <- yearly_mean_site_test_3 %>% 
  group_by(Study_ID, response_var, Climate, year) %>%
  mutate(lnR1 = log(Xe1/Xc),
         lnR2 = log(Xe2/Xc),
         lnR3 = log(Xe3/Xc),
         lnR4 = log(Xe4/Xc),
         lnR5 = log(Xe5/Xc))

# This is doing the same thing as chunck before where we are creating a column header for each individual site but doing it for the studies that have multiple controls. 
yearly_mean_hauer_hickenbottom <- yearly_mean_site %>%
  filter(Study_ID == "Hauer & Spencer 1998" | Study_ID == "Hickenbottom et al. 2023") %>%
  mutate(TempNorm = as.numeric(TempNorm))



hauer_hickenbottom_test <- yearly_mean_hauer_hickenbottom %>%
  group_by(Study_ID, Pair, year, Burn_Unburn, response_var, Climate) %>%
  dplyr::summarize(Xe = sum(TempNorm[Burn_Unburn == "Burn"], na.rm = TRUE),
                   Xc = sum(TempNorm[Burn_Unburn == "Unburn"], na.rm = TRUE)) # Creating a control and treatment column for the temporal normalization 

yearly_multiple_control <- hauer_hickenbottom_test %>% 
  group_by(Study_ID, response_var, Climate, year) %>%
  summarize(Xe1 = sum(Xe[Pair == "Site_1"], na.rm = TRUE),
            Xe2 = sum(Xe[Pair == "Site_2"], na.rm = TRUE),
            Xe3 = sum(Xe[Pair == "Site_3"], na.rm = TRUE),
            Xe4 = sum(Xe[Pair == "Site_4"], na.rm = TRUE),
            Xe5 = sum(Xe[Pair == "Site_5"], na.rm = TRUE),
            Xc345 = sum(Xc[Pair == "Control_3_4_5"], na.rm = TRUE),
            Xc2 = sum(Xc[Pair == "Control_2"], na.rm = TRUE),
            Xc = sum(Xc[Pair == "Control_1"], na.rm = TRUE)) 


yearly_effect_multiple <- yearly_multiple_control %>% 
  group_by(Study_ID, response_var, Climate, year) %>%
  mutate(lnR1 = log(Xe1/Xc),
         lnR2 = log(Xe2/Xc2),
         lnR3 = log(Xe3/Xc345),
         lnR4 = log(Xe4/Xc345),
         lnR5 = log(Xe5/Xc345)) %>% 
  ungroup()

# Adding columns to match brie_test_stats
yearly_effect_size <-yearly_effect_size %>% 
  add_column(Xc345 = NA,
             Xc2 = NA) %>% 
  ungroup()

# combining Hauer and Hickenbottom back into the mix 
yearly_effect_size_combine <- yearly_effect_size %>% 
  add_row(yearly_effect_multiple)

# Adding in more broad climate 
unique(yearly_effect_size_combine$Climate)

yearly_effect_size_combine <- yearly_effect_size_combine %>% 
  mutate(yearly_effect_size_combine, Biome = ifelse(grepl("Dfb|Dfc|Dsb", Climate), "Cold",
                              ifelse(grepl("Cfb|Csa|Csb|Cfa", Climate), "Temperate",
                              ifelse(grepl("BSk", Climate), "Arid",
                              ifelse(grepl("As|Af|Am", Climate), "Tropical",
                              ifelse(grepl("ET|EF", Climate), "Polar", "Other"))))))

# pivoting to make the responses in one column
# effect size 
yearly_effect_size_combine_long <- yearly_effect_size_combine %>% 
  pivot_longer(
    cols = lnR1:lnR5,
    names_to = "Site",
    values_to = "Effect_size",
    values_drop_na = TRUE
  ) %>% 
  filter(is.finite(Effect_size))
```

# Add in corrected Crandall and papers that are greater than 5 years to this list 
```{r}
# combine the yearly_effect_size_combine_long with Crandall, Abbott, and Rhea. 

Crandall_effect_size <- read_csv(here("Output_for_analysis", "Crandall_effect_size.csv")) %>% 
  select(-...1)

abbott_effect_size <- read_csv(here("Output_for_analysis", "Abbott_effect_size.csv")) %>% 
  select(-...1)

rhea_effect_size <- read_csv(here("Output_for_analysis", "Rhea_effect_size.csv")) %>% 
  select(-...1)


# Adding columns to yearly_effect_size_combine_long to match Crandall_effect_size and above_five_effect_size 
yearly_effect_size_combine_long <-yearly_effect_size_combine_long %>% 
  add_column(Xc34 = NA) %>% 
  ungroup() %>% 
  select(Study_ID, response_var, Climate, year, Xe1, Xe2, Xe3, Xe4, Xe5, Xc, Xc2, Xc34, Xc345, Biome, Site, Effect_size)

# Adding columns to Crandall_effect_size to match yearly_effect_size_combine_long and above_five_effect_size 
Crandall_effect_size <-Crandall_effect_size %>% 
  add_column(Time_Since_Fire = as.numeric(0),
             Biome = "Temperate",
             Xe5 = NA,
             Xc345 = NA) %>% 
  ungroup() %>% 
  select(Study_ID, response_var, Climate, year, Xe1, Xe2, Xe3, Xe4, Xe5, Xc, Xc2, Xc34, Xc345, Biome, Site, Effect_size) 

# Adding columns to abbott_effect_size to match yearly_effect_size_combine_long and Crandall_effect_size 
abbott_effect_size <-abbott_effect_size %>% 
  add_column(Xe2 = NA, 
             Xe3 = NA, 
             Xe4 = NA, 
             Xe5 = NA, 
             Xc2 = NA, 
             Xc34 = NA,
             Xc345 = NA, 
             Biome = NA) %>% 
  ungroup() %>% 
  select(Study_ID, response_var, Climate, year, Xe1, Xe2, Xe3, Xe4, Xe5, Xc, Xc2, Xc34, Xc345, Biome, Site, Effect_size) 

# Adding columns to rhea_effect_size to match abbott_effect_size, yearly_effect_size_combine_long and Crandall_effect_size 
rhea_effect_size <-rhea_effect_size %>% 
  add_column(Xe4 = NA, 
             Xe5 = NA, 
             Xc2 = NA, 
             Xc34 = NA,
             Xc345 = NA, 
             Biome = NA) %>% 
  ungroup() %>% 
  select(Study_ID, response_var, Climate, year, Xe1, Xe2, Xe3, Xe4, Xe5, Xc, Xc2, Xc34, Xc345, Biome, Site, Effect_size) 


# combining all of them back into the mix back into the mix 
yearly_effect_size_combine_long <- yearly_effect_size_combine_long %>% 
  add_row(Crandall_effect_size) %>% 
  add_row(abbott_effect_size) %>% 
  add_row(rhea_effect_size)


### LOOKS LIKE THE TIME SINCE FIRE AND YEARS ARENT PAIRING UP LIKE I WANT THEM TO ####
# Creating a year column with a dataframe that has TSF accurately 
TSF_year <- control_impact %>% 
  mutate(year = year(Sampling_Date)) %>% 
  select(Study_ID, Time_Since_Fire, year) %>% 
  distinct()

# Joining the TSF and the yearly effect size combine long dataframe 
yearly_effect_size_combine_long <- left_join(yearly_effect_size_combine_long, TSF_year, by = c("Study_ID", "year")) %>% 
  distinct(Effect_size, .keep_all = TRUE)

# Filling in the missing Time Since Fire values because they were originally excluded in the above code 
yearly_effect_size_combine_long <- yearly_effect_size_combine_long %>% 
  mutate(Time_Since_Fire = case_when(Study_ID == 'Abbott et al. 2021' & year == "2017" ~ ">10",
                                     Study_ID == 'Abbott et al. 2021' & year == "2018" ~ ">10",
                                     Study_ID == 'Hauer & Spencer 1998' & year == "1992" ~ "4",
                                     Study_ID == 'Rhea et al. 2021' ~ ">10",
                                     Study_ID == 'Crandall et al. 2021' ~ "0",
                                  TRUE ~ Time_Since_Fire))

yearly_effect_size_combine_long <- yearly_effect_size_combine_long %>% 
  mutate(Biome = case_when(Study_ID == 'Abbott et al. 2021' ~ "Cold",
                           Study_ID == 'Rhea et al. 2021' ~ "Cold",
                         TRUE ~ Biome))

write_csv(yearly_effect_size_combine_long, here("Output_for_analysis", "Effect_Size.csv"))

```


# Calculating a confidence interval for all of the effect sizes
```{r}
# calculating the mean percent_difference by climate
climate_summary_effect <- yearly_effect_size_combine_long %>%
  group_by(Climate, response_var, Biome) %>%
  summarise(mean_effect = mean(Effect_size, na.rm = TRUE),
            sd_effect = sd(Effect_size, na.rm = TRUE),
            n_effect = n(),
            Effect_size = mean(Effect_size, na.rm = TRUE)) %>%
  mutate(se_effect = sd_effect / sqrt(n_effect),
         lower_ci_effect = mean_effect - qt(1 - (0.05 / 2), n_effect - 1) * se_effect,
         upper_ci_effect = mean_effect + qt(1 - (0.05 / 2), n_effect - 1) * se_effect)


# Calculating a confidence interval for all of the effect sizes
# calculating the mean effect_size by climate
TSF_summary_effect <- yearly_effect_size_combine_long %>%
  group_by(Time_Since_Fire, response_var) %>%
  summarise(mean_effect = mean(Effect_size, na.rm = TRUE),
            sd_effect = sd(Effect_size, na.rm = TRUE),
            n_effect = n(),
            Effect_size = mean(Effect_size, na.rm = TRUE)) %>%
  mutate(se_effect = sd_effect / sqrt(n_effect),
         lower_ci_effect = mean_effect - qt(1 - (0.05 / 2), n_effect - 1) * se_effect,
         upper_ci_effect = mean_effect + qt(1 - (0.05 / 2), n_effect - 1) * se_effect)


```




# Plotting
```{r - plotting effect size }
# Ploting #
vn = expression(paste(""*N*O[3]^"-"))

# within 5 years #
under_ten <- yearly_effect_size_combine_long %>% 
  filter(!Time_Since_Fire == ">10") 

no_dwc <- climate_summary_effect %>% 
  filter(!Climate == "Dwc",
         !Climate == "Dfc")  # Removing the abbott and Rhea paper that is greater than 5 years from TSF

# Count how many watersheds are being included for the climate analysis #
under_ten_long <- under_ten %>% 
  pivot_longer(
    cols = Xe1:Xc345,
    names_to = "Watershed",
    values_to = "Mean",
    values_drop_na = TRUE
  ) %>% 
  filter(is.finite(Effect_size))

under_ten_long <- under_ten_long %>% 
  group_by(Study_ID) %>% 
  distinct(Watershed, .keep_all = T) %>% 
  filter(Mean != 0) %>% 
  dplyr::select(Study_ID, Watershed, Mean) 
  

count_climate <- under_ten_long %>% 
  count(Study_ID)
sum(count_climate$n)

write_csv(count_climate, "~/GitHub/rc_sfa-rc-3-wenas-meta/FireMeta_Rproj/Output_for_analysis/Climate_count.csv")


# 58 watersheds are included in the climate analysis 

# geom_jitter for climate for studies within 5 years # 
ggplot(under_ten, aes(Effect_size, Climate, color = response_var),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0.4, size = 3) +
  geom_pointrange(aes(xmin = lower_ci_effect, xmax = upper_ci_effect,
                      color = response_var),
                  position = position_dodge(width = -0.5), size = 1.5, data = no_dwc) +
  geom_vline(xintercept = 0, linewidth = 0.5, color = "red") +
  xlim(-6.5, 8.5) +
  ylab("Koppen (Climate Classification)") +
  xlab("Effect Size") +
  geom_segment(aes(x = 6.25, xend = 6.25, y = 8.5, yend = 5.5), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 8.5, yend = 8.5), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 5.5, yend = 5.5), color = "black") +
  
  geom_segment(aes(x = 6.25, xend = 6.25, y = 5.3, yend = 1.5), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 5.3, yend = 5.3), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 1.5, yend = 1.5), color = "black") +
  
  geom_segment(aes(x = 6.25, xend = 6.25, y = 1.3, yend = 0.7), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 1.3, yend = 1.3), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 0.7, yend = 0.7), color = "black") +
  annotate("text",
           x = c(7.75, 7.75, 7),
           y = c(7, 3.5, 1),
           label = c("Continental", "Temperate", "Arid"),
           family = "", fontface = "bold") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = "Analyte"),
                     labels = c('DOC', vn)) +
  stat_summary(fun.data = give.n, geom = "text", fun.y = median,
                  position = position_nudge(x = 3.5)) +
  scale_y_discrete(labels=c("Dsb" = "Mediterranean", 
                            "Dfc" = "Subarctic",
                            "Dfb" = "Warm-humid",
                            "Csb" = "Warm-Mediterranean",
                            "Csa" = "Hot-Mediterranean",
                            "Cfb" = "Subtropical highland",
                            "Cfa" = "Humid subtropical",
                            "BSk" = "Cold semi-arid")) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 30),
        axis.text.y = element_text(size = 15),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 35),
        legend.text = element_text(size = 30),
        legend.position = c(0.17, 0.80))
  

ggsave("Effect_Size.climate.pdf",
       path = ("~/GitHub/rc_sfa-rc-3-wenas-meta/FireMeta_Rproj/initial_plots/03_Wenas_effect_size"),
       width = 10, height = 8, units = "in")



# geom_jitter for TSF # 
# Reordering for plotting purposes
yearly_effect_size_combine_long$Time_Since_Fire <- factor(yearly_effect_size_combine_long$Time_Since_Fire, levels = c("0", "1", "2", "3", "4", "5", ">10"))

# Count how many watersheds are being included for the climate analysis #
TSF_long <- yearly_effect_size_combine_long %>% 
  pivot_longer(
    cols = Xe1:Xc345,
    names_to = "Watershed",
    values_to = "Mean",
    values_drop_na = TRUE
  ) %>% 
  filter(is.finite(Effect_size))

TSF_long <- TSF_long %>% 
  group_by(Study_ID) %>% 
  distinct(Watershed, .keep_all = T) %>% 
  filter(Mean != 0) %>% 
  dplyr::select(Study_ID, Watershed, Mean)

count_TSF <- TSF_long %>% 
  count(Study_ID)
sum(count_TSF$n)

write_csv(count_TSF, "~/GitHub/rc_sfa-rc-3-wenas-meta/FireMeta_Rproj/Output_for_analysis/TSF_count.csv")

# 58 watersheds are included in the climate analysis 

 


ggplot(yearly_effect_size_combine_long, aes(Time_Since_Fire, Effect_size, color = response_var),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0.4, size = 3) +
  geom_pointrange(aes(ymin = lower_ci_effect, ymax = upper_ci_effect,
                      color = response_var),
                  position = position_dodge(width = -0.5), size = 1.5, data = TSF_summary_effect) +
  geom_hline(yintercept = 0, linewidth = 0.5, color = "red") +
  ylim(-3, 4) +
  ylab("Effect Size") +
  xlab("Year since Fire") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = "Analyte"),
                     labels = c('DOC', vn)) +
  stat_summary(fun.data = give.n, geom = "text", fun.y = median,
                  position = position_nudge(x = -0.4)) +
  scale_x_discrete(labels=c("0" = "Year of Fire", "1" = "1-2",
                            "2" = "2-3", "3" = "3-4", "4" = "4-5",
                            "5" = "5-6", ">10" = ">10")) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 30),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 35),
        legend.text = element_text(size = 30),
        legend.position = c(0.17, 0.90))

ggsave("Effect_Size_TSF_Horizontal.pdf",
       path = here("initial_plots", "03_Wenas_effect_size"),
       width = 10, height = 8, units = "in")



```


```{r AGU plots}
vn = expression(paste(""*N*O[3]^"-"))

# within 5 years #
under_ten <- yearly_effect_size_combine_long %>% 
  filter(!Time_Since_Fire == ">10") 

no_dwc <- climate_summary_effect %>% 
  filter(!Climate == "Dwc")

ggplot(under_ten, aes(Effect_size, Climate, color = response_var),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0, size = 3) +
  geom_pointrange(aes(xmin = lower_ci_effect, xmax = upper_ci_effect,
                      color = response_var),
                  position = position_dodge(width = -0.5), size = 1.5, data = no_dwc) +
  geom_vline(xintercept = 0, linewidth = 1, color = "red") +
  xlim(-5, 8.5) +
  ylab("Koppen (Climate Classification)") +
  xlab("Effect Size") +
  geom_segment(aes(x = 6.25, xend = 6.25, y = 8.5, yend = 5.5), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 8.5, yend = 8.5), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 5.5, yend = 5.5), color = "black") +
  
  geom_segment(aes(x = 6.25, xend = 6.25, y = 5.3, yend = 1.5), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 5.3, yend = 5.3), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 1.5, yend = 1.5), color = "black") +
  
  geom_segment(aes(x = 6.25, xend = 6.25, y = 1.3, yend = 0.7), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 1.3, yend = 1.3), color = "black") +
  geom_segment(aes(x = 5.5, xend = 6.25, y = 0.7, yend = 0.7), color = "black") +
  annotate("text",
           x = c(7.75, 7.75, 7),
           y = c(7, 3.5, 1),
           label = c("Continental", "Temperate", "Arid"),
           family = "", fontface = "bold") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = ""),
                     labels = c('DOC', vn)) +
  
  scale_y_discrete(labels=c("Dsb" = "Mediterranean", 
                            "Dfc" = "Subarctic",
                            "Dfb" = "Warm-humid",
                            "Csb" = "Warm-Mediterranean",
                            "Csa" = "Hot-Mediterranean",
                            "Cfb" = "Subtropical highland",
                            "Cfa" = "Humid subtropical",
                            "BSk" = "Cold semi-arid")) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 30),
        axis.text.y = element_text(size = 15),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 35),
        legend.text = element_text(size = 30),
        legend.position = c(0.17, 0.80))
  

ggsave("AGU_Effect_Size_climate.pdf",
       path = here("initial_plots", "AGU"),
       width = 10, height = 8,  units = "in")



# TSF #
yearly_effect_size_combine_long$Time_Since_Fire <- factor(yearly_effect_size_combine_long$Time_Since_Fire, levels = c("0", "1", "2", "3", "4", "5", ">10"))

ggplot(yearly_effect_size_combine_long, aes(Time_Since_Fire, Effect_size, color = response_var),
       position = position_dodge(width = -0.5)) +
  geom_jitter(position = position_jitter(0.1), alpha = 0, size = 3) +
  geom_pointrange(aes(ymin = lower_ci_effect, ymax = upper_ci_effect,
                      color = response_var),
                  position = position_dodge(width = -0.5), size = 1.5, data = TSF_summary_effect) +
  geom_hline(yintercept = 0, linewidth = 1, color = "red") +
  ylim(-3, 4) +
  ylab("Effect Size") +
  xlab("Year since Fire") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"),
                     guide = guide_legend(title = ""),
                     labels = c('DOC', vn)) +
  
  scale_x_discrete(labels=c("0" = "Year of Fire", "1" = "1-2",
                            "2" = "2-3", "3" = "3-4", "4" = "4-5",
                            "5" = "5-6", ">10" = ">10")) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 30),
        axis.title.x = element_text(size = 40),
        axis.title.y = element_text(size = 35),
        legend.text = element_text(size = 30),
        legend.position = c(0.17, 0.90))

ggsave("AGU_Effect_Size_TSF.pdf",
       path = here("initial_plots", "AGU"),
       width = 10, height = 8,  units = "in")
#


```







































```{r - code graveyard}
# # Fill gaps
# control_impact_gap_fill <- control_impact %>% 
#   group_by(Study_ID) %>% 
#   complete(Sampling_Date = seq.Date(min(Sampling_Date), max(Sampling_Date), by = "day")) %>% 
#   mutate(NO3 = na.approx(NO3))  
#   
# 
# df <- tibble(
#   Group = c("A", "A", "A", "B", "B"),
#   Date = as.Date(c("2023-10-01", "2023-10-02", "2023-10-04", "2023-10-01", "2023-10-03")),
#   Value = c(10, 15, 20, 5, 10)
# )
# 
# df_filled <- df %>%
#   group_by(Group) %>%
#   complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
#   fill(Value, .direction = "up")


# control_impact_summary <- control_impact_fill %>% 
#   group_by(Study_ID, Site, year, month) %>% 
#   summarise(XeNO3 = mean(NO3_Interp[Burn_Unburn == "Burn"], na.rm = TRUE),
#             XcNO3 = mean(NO3_Interp[Burn_Unburn == "Unburn"], na.rm = TRUE),
#             XeDOC = mean(DOC_Interp[Burn_Unburn == "Burn"], na.rm = TRUE),
#             XcDOC = mean(DOC_Interp[Burn_Unburn == "Unburn"], na.rm = TRUE))
# 
# 
# 
#   
# gerla_month_mean_burn <- gerla_burn_fill %>%
#   group_by(Climate, year, month) %>%
#   summarize(Xe = mean(NO3[Burn_Unburn == "Burn"], na.rm = TRUE),
#             TimeNorm_Burn = Xe * 6423/100)


# MeanConcTest <- metadata_long %>%
#   group_by(Study_ID, Pair, response_var) %>%
#   summarize(Ne = sum(Burn_Unburn == "Burn"),
#                    Nc = sum(Burn_Unburn == "Unburn"),
#                    Xe = mean(mean_concentration[Burn_Unburn == "Burn"], na.rm = TRUE),
#                    Xc = mean(mean_concentration[Burn_Unburn == "Unburn"], na.rm = TRUE))
# 
# # This is calculating the mean for EACH site within a study_ID # 
#     # Any study_ID that doesn't have up to 10 sites will have a 0 in the respective Xen column 
# analyte_table_means
# 
# 
# analyte_table_means <- MeanConcTest %>%
#   select(-Pair) %>%
#   group_by(Study_ID, response_var) %>%
#   summarize(Ne = sum(Ne),
#             Nc = sum(Nc),
#             Xe1 = sum(Xe[Pair == "Site_1"], na.rm = TRUE),
#             Xe2 = sum(Xe[Pair == "Site_2"], na.rm = TRUE),
#             Xe3 = sum(Xe[Pair == "Site_3"], na.rm = TRUE),
#             Xe4 = sum(Xe[Pair == "Site_4"], na.rm = TRUE),
#             Xe5 = sum(Xe[Pair == "Site_5"], na.rm = TRUE),
#             Xe6 = sum(Xe[Pair == "Site_6"], na.rm = TRUE),
#             Xe7 = sum(Xe[Pair == "Site_7"], na.rm = TRUE),
#             Xe8 = sum(Xe[Pair == "Site_8"], na.rm = TRUE),
#             Xe9 = sum(Xe[Pair == "Site_9"], na.rm = TRUE),
#             Xe10 = sum(Xe[Pair == "Site_10"], na.rm = TRUE),
#             Xe11 = sum(Xe[Pair == "Site_11"], na.rm = TRUE),
#             Xe12 = sum(Xe[Pair == "Site_12"], na.rm = TRUE),
#             Xe13 = sum(Xe[Pair == "Site_13"], na.rm = TRUE),
#             Xe14 = sum(Xe[Pair == "Site_14"], na.rm = TRUE),
#             Xe15 = sum(Xe[Pair == "Site_15"], na.rm = TRUE),
#             Xe16 = sum(Xe[Pair == "Site_16"], na.rm = TRUE),
#             Xe17 = sum(Xe[Pair == "Site_17"], na.rm = TRUE),
#             Xe18 = sum(Xe[Pair == "Site_18"], na.rm = TRUE),
#             Xe19 = sum(Xe[Pair == "Site_19"], na.rm = TRUE),
#             Xe20 = sum(Xe[Pair == "Site_20"], na.rm = TRUE),
#             Xe21 = sum(Xe[Pair == "Site_21"], na.rm = TRUE),
#             Xe22 = sum(Xe[Pair == "Site_22"], na.rm = TRUE),
#             Xc = sum(Xc[Pair == "Control"], na.rm = TRUE),
#             Xc1 = sum(Xc[Pair == "Control_1"], na.rm = TRUE),
#             Xc2 = sum(Xc[Pair == "Control_2"], na.rm = TRUE),
#             Xc345 = sum(Xc[Pair == "Control_3_4_5"], na.rm = TRUE),
#             ) 
# 
# # making all the cells with 0 due to no sites beyond what the study had NAs to visualize the dataframe a little better. 
# analyte_table_means[analyte_table_means == 0] <- NA
# 
# # check here to make sure this is calculating what you want it to calculate. 
# #lets filter out Coombs and Melack here to check 
# coombs_fill_check <- control_impact_fill %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# burd_fill_check <- analyte_table_means %>% 
#   filter(Study_ID == "Burd et al 2018")
# 
# write_csv(coombs_fill_check, here("Output_for_analysis", "coombs_fill_check.csv"))
# 
# coombs_long_check <- metadata_long %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# coombs_mean_check <- MeanConcTest %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# coombs_summary_check <- analyte_table_means %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")
# 
# 
# coombs_check <- metadata_long %>% 
#   filter(Study_ID == "Coombs & Melack, 2013")


# Abbott test #
# abbott <- control_impact %>% 
#   filter(Study_ID == "Abbott et al. 2021")
# 
# abbott_time_fill <- abbott %>% 
#   complete(Sampling_Date = seq.Date(min(Sampling_Date), max(Sampling_Date), by = "day")) %>% 
#   select("Sampling_Date", "Site", "DOC", "NO3") 
# 
# ggplot(abbott_time_fill, aes(x = Sampling_Date, y = NO3)) +
#   geom_point() +
#   facet_wrap(~Site, scales = "free") +
#   theme_bw()
# 
# 
# abbott_time_fill_filter <- abbott_time_fill %>% 
#   filter(Site %in% c("BS.11", "BS.12"))
# 
# abbott_time_fill_filter <- abbott_time_fill_filter %>% 
#   fill_gaps(date_col = 1, frequency = '1 day',
#             groups = 'Site') %>% 
#   mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
#          DOC_Interp = na.approx(DOC, na.rm = FALSE))
# 
# 
#   
# abbott_solute_fill <-  abbott_time_fill %>%
#   fill_gaps(date_col = 1, frequency = '1 day',
#             groups = 'Site') %>% 
#   mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
#          DOC_Interp = na.approx(DOC, na.rm = FALSE))
# 
# ggplot(abbott_solute_fill, aes(x = Sampling_Date, y = NO3_Interp)) +
#   geom_point() +
#   facet_wrap(~Site, scales = "free") +
#   theme_bw()


# lets try to do this for 2 studies # 
# multiple <- control_impact %>% 
#   filter(Study_ID %in% c("Abbott et al. 2021", "Burd et al 2018"))
# 
# multiple_time_fill <- multiple %>% 
#   select("Study_ID", "Sampling_Date", "Site", "DOC", "NO3") 
# 
# multiple_time_fill <- multiple_time_fill %>%
#   group_by(Study_ID) %>% 
#   fill_gaps(date_col = 2, frequency = '1 day',
#             groups = 'Site') %>% 
#   mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
#          DOC_Interp = na.approx(DOC, na.rm = FALSE))


# # Hauer test #
# hauer <- control_impact %>% 
#   filter(Study_ID == "Hauer & Spencer 1998")
# 
# hauer_time_fill <- hauer %>% 
#   select("Sampling_Date", "Site", "NO3") 
# 
# hauer_time_fill_filter <- hauer_time_fill %>% 
#   group_by(Site) %>% 
#   complete(Sampling_Date = seq(min(Sampling_Date), max(Sampling_Date), by = "1 day"))
# 
# 
# 
# ggplot(coombs_time_fill_filter, aes(x = Sampling_Date, y = NO3_Interp)) +
#   geom_point() +
#   facet_wrap(~Site, scales = "free") +
#   theme_bw()
```











