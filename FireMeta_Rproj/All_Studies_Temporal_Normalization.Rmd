---
title: "All_Studies_Temporal_Normalization"
output: html_document
date: "2023-10-19"
editor_options: 
  chunk_output_type: console
---

The purpose of this script is to read in the studies from the final meta data list and to normalize daily stream concentrations by time. 

Script Workflow:

Step 1) Load in all the individual studies from "meta_final" located within inputs in the repository.  

Step 2) Filter out studies that are Pre_Post -> we only want papers that are Control_vs_Impact

Step 3) run script that fills in concentrations for missing days in between discrete sampling days and then takes an average by month. 


# Status: in progress

# ==============================================================================
# Author: Jake Cavaiani 
# 18 October 2023
# ==============================================================================

## Load packages and set working directory
```{r Jake/Mac}
#for Jake/mac

rm(list=ls(all=T)) #this clears your Environment


library(tidyverse)
library(here)
library(forecastML)
library(zoo)
library(hrbrthemes)
library(viridis)

```

```{r data prepper}
studies_file_list <- list.files(path = "inputs/Studies/meta_final/", 
                                  recursive=F, 
                                  pattern=".csv", 
                                  full.names=TRUE)

storm_list_beta<-do.call("list", lapply(studies_file_list, 
                                        read.csv, 
                                        stringsAsFactors=FALSE, 
                                        header=T))

meta_df <-do.call("rbind", lapply(studies_file_list, 
                                     read.csv, 
                                     check.names = FALSE,
                                     stringsAsFactors=FALSE, 
                                     header=T, blank.lines.skip = TRUE, fill=TRUE)) 

unitconv=read.csv("inputs/UnitConversion.csv",stringsAsFactors = FALSE)


```


```{r data prep}
# Filtering by only the control vs.impact studies because the pre_post are going to go through a different calculation 
# Also filtering out the studies where I don't have exact sampling days 
control_impact <- meta_df %>% 
  filter(`Design (Control/Burn; Pre/Post)` == "Control_Reference_vs_Impact",
         Study_ID != "Caldwell et al. 2020",
         Study_ID != "Stephan et al. 2015",
         Study_ID != "Writer & Murphy, 2012", 
         Mean_Median_or_IndividualSample == "Individual")

# separate out the time columns 
time_format <- control_impact %>% 
  select(Study_ID, Sampling_Date) 

# Make them all the same format and then merge back together in one column
time_format <- time_format %>% 
  mutate(DateTime = mdy(Sampling_Date),
         DateTime2 = mdy_hm(Sampling_Date), 
         DateTime2 = as.Date(DateTime2)) %>% 
  select(-Sampling_Date) %>% 
  mutate(Sampling_Date = coalesce(DateTime, DateTime2),
         Sampling_Date = as.character(Sampling_Date)) %>% 
  select(-DateTime, -DateTime2) 

# merge this dataframe with the metadata df to make sure all the dates are the same format and there aren't any NAs
control_impact <- control_impact %>% 
  mutate(Sampling_Date = time_format$Sampling_Date)


# Creating a year month and day separate column
control_impact <- control_impact %>% 
  mutate(Sampling_Date = ymd(Sampling_Date),
         year = year(Sampling_Date),
         month = month(Sampling_Date),
         day = day(Sampling_Date))

# changing the structure of the concentrations
control_impact <- control_impact %>% 
  mutate(DOC = as.numeric(DOC),
         NO3 = as.numeric(NO3))

# Changing the units to make sure everything is consistent in mg_N_L or mg_C_L
control_impact_units <- control_impact %>% 
  mutate(Area_watershed_km = case_when(Area_unit == 'ha' ~ Area_watershed * .01,
                                       Area_unit == 'km' ~ print(Area_watershed)),
         
         DOC_mg_C_L = case_when(DOC_unit == 'mg_C_L' ~ print(DOC),
                                DOC_unit == 'mg_L' ~ print(DOC),
                                DOC_unit == 'um' ~ print(DOC)),
         
         NO3_mg_N_L = case_when(NO3_unit == 'um' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'umol_NO2_NO3_L' ~ NO3 * 0.014007000,
                                NO3_unit == 'mg_N_L' ~ print(NO3),
                                NO3_unit == 'ug_N_L' ~ NO3 * .001, 
                                NO3_unit == 'mg_L' ~ NO3 * 0.225904780))


```

```{r - Abbott test}
# Abbott test #
abbott <- control_impact %>% 
  filter(Study_ID == "Abbott et al. 2021")

abbott_time_fill <- abbott %>% 
  complete(Sampling_Date = seq.Date(min(Sampling_Date), max(Sampling_Date), by = "day")) %>% 
  select("Sampling_Date", "Site", "DOC", "NO3") 

ggplot(abbott_time_fill, aes(x = Sampling_Date, y = NO3)) +
  geom_point() +
  facet_wrap(~Site, scales = "free") +
  theme_bw()


abbott_time_fill_filter <- abbott_time_fill %>% 
  filter(Site %in% c("BS.11", "BS.12"))

abbott_time_fill_filter <- abbott_time_fill_filter %>% 
  fill_gaps(date_col = 1, frequency = '1 day',
            groups = 'Site') %>% 
  mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
         DOC_Interp = na.approx(DOC, na.rm = FALSE))


  
abbott_solute_fill <-  abbott_time_fill %>%
  fill_gaps(date_col = 1, frequency = '1 day',
            groups = 'Site') %>% 
  mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
         DOC_Interp = na.approx(DOC, na.rm = FALSE))

ggplot(abbott_solute_fill, aes(x = Sampling_Date, y = NO3_Interp)) +
  geom_point() +
  facet_wrap(~Site, scales = "free") +
  theme_bw()

```


```{r 2 study test}
# lets try to do this for 2 studies # 
multiple <- control_impact %>% 
  filter(Study_ID %in% c("Abbott et al. 2021", "Burd et al 2018"))

multiple_time_fill <- multiple %>% 
  select("Study_ID", "Sampling_Date", "Site", "DOC", "NO3") 

multiple_time_fill <- multiple_time_fill %>%
  group_by(Study_ID) %>% 
  fill_gaps(date_col = 2, frequency = '1 day',
            groups = 'Site') %>% 
  mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
         DOC_Interp = na.approx(DOC, na.rm = FALSE))

```

```{r - Coombs test}
# Coombs test #
coombs <- control_impact %>% 
  filter(Study_ID == "Coombs & Melack, 2013")

coombs_time_fill <- coombs %>% 
  select("Sampling_Date", "Site", "DOC", "NO3") 

coombs_time_fill_filter <- coombs_time_fill %>% 
  fill_gaps(date_col = 1, frequency = '1 day',
            groups = 'Site') %>% 
  mutate(NO3_Interp = na.approx(NO3, na.rm = FALSE),
         DOC_Interp = na.approx(DOC, na.rm = FALSE))


ggplot(coombs_time_fill_filter, aes(x = Sampling_Date, y = NO3_Interp)) +
  geom_point() +
  facet_wrap(~Site, scales = "free") +
  theme_bw()

```


```{r - all the studies}
# Okay lets do this for all the studies # 
# filling in daily time series and interpolating by study and site
control_impact_fill <-  control_impact_units %>% 
  select("Study_ID", "Latitude", "Longitude", "Area_watershed_km", "Pair", "Climate", "Site", "Burn_Unburn", "Time_Since_Fire", "Sampling_Date", "DOC_mg_C_L", "NO3_mg_N_L", "year", "month", "day") %>% 
  group_by(Study_ID) %>% 
  fill_gaps(date_col = 10, frequency = '1 day',
            groups = 'Site') %>% 
  mutate(NO3_Interp = na.approx(NO3_mg_N_L, na.rm = FALSE),
         DOC_Interp = na.approx(DOC_mg_C_L, na.rm = FALSE)) %>% 
  fill(Study_ID:Time_Since_Fire, year:day)
  
```


```{r}
# This is code from my other script. I am going to try this way to see if this is what we want to get to 
# pivoting to make the responses in one column
metadata_long <- control_impact_fill %>%
  pivot_longer(
    cols = NO3_Interp:DOC_Interp,
    names_to = "response_var",
    values_to = "mean_concentration",
    values_drop_na = TRUE
  )

MeanConcTest <- metadata_long %>%
  group_by(Study_ID, Pair, Burn_Unburn, response_var, Climate, Time_Since_Fire) %>%
  dplyr::summarize(Ne = sum(Burn_Unburn == "Burn"),
                   Nc = sum(Burn_Unburn == "Unburn"),
                   Xe = mean(mean_concentration[Burn_Unburn == "Burn"], na.rm = TRUE),
                   Xc = mean(mean_concentration[Burn_Unburn == "Unburn"], na.rm = TRUE))

# This is calculating the mean for EACH site within a study_ID # 
    # Any study_ID that doesn't have up to 10 sites will have a 0 in the respective Xen column 
analyte_table_means <- MeanConcTest %>%
  select(-Pair) %>%
  group_by(Study_ID, response_var, Climate, Time_Since_Fire) %>%
  summarize(Ne = sum(Ne),
            Nc = sum(Nc),
            Xe1 = sum(Xe[Pair == "Site_1"], na.rm = TRUE),
            Xe2 = sum(Xe[Pair == "Site_2"], na.rm = TRUE),
            Xe3 = sum(Xe[Pair == "Site_3"], na.rm = TRUE),
            Xe4 = sum(Xe[Pair == "Site_4"], na.rm = TRUE),
            Xe5 = sum(Xe[Pair == "Site_5"], na.rm = TRUE),
            Xe6 = sum(Xe[Pair == "Site_6"], na.rm = TRUE),
            Xe7 = sum(Xe[Pair == "Site_7"], na.rm = TRUE),
            Xe8 = sum(Xe[Pair == "Site_8"], na.rm = TRUE),
            Xe9 = sum(Xe[Pair == "Site_9"], na.rm = TRUE),
            Xe10 = sum(Xe[Pair == "Site_10"], na.rm = TRUE),
            Xe11 = sum(Xe[Pair == "Site_11"], na.rm = TRUE),
            Xe12 = sum(Xe[Pair == "Site_12"], na.rm = TRUE),
            Xe13 = sum(Xe[Pair == "Site_13"], na.rm = TRUE),
            Xe14 = sum(Xe[Pair == "Site_14"], na.rm = TRUE),
            Xe15 = sum(Xe[Pair == "Site_15"], na.rm = TRUE),
            Xe16 = sum(Xe[Pair == "Site_16"], na.rm = TRUE),
            Xe17 = sum(Xe[Pair == "Site_17"], na.rm = TRUE),
            Xe18 = sum(Xe[Pair == "Site_18"], na.rm = TRUE),
            Xe19 = sum(Xe[Pair == "Site_19"], na.rm = TRUE),
            Xe20 = sum(Xe[Pair == "Site_20"], na.rm = TRUE),
            Xe21 = sum(Xe[Pair == "Site_21"], na.rm = TRUE),
            Xe22 = sum(Xe[Pair == "Site_22"], na.rm = TRUE),
            Xc = sum(Xc[Pair == "Control"], na.rm = TRUE),
            Xc1 = sum(Xc[Pair == "Control_1"], na.rm = TRUE),
            Xc2 = sum(Xc[Pair == "Control_2"], na.rm = TRUE),
            Xc345 = sum(Xc[Pair == "Control_3_4_5"], na.rm = TRUE),
            ) 

# making all the cells with 0 due to no sites beyond what the study had NAs to visualize the dataframe a little better. 
analyte_table_means[analyte_table_means == 0] <- NA

# check here to make sure this is calculating what you want it to calculate. 
#lets filter out Coombs and Melack here to check 
coombs_fill_check <- control_impact_fill %>% 
  filter(Study_ID == "Coombs & Melack, 2013")

burd_fill_check <- control_impact_fill %>% 
  filter(Study_ID == "Burd et al 2018")

write_csv(coombs_fill_check, here("Output_for_analysis", "coombs_fill_check.csv"))

coombs_long_check <- metadata_long %>% 
  filter(Study_ID == "Coombs & Melack, 2013")

coombs_mean_check <- MeanConcTest %>% 
  filter(Study_ID == "Coombs & Melack, 2013")

coombs_summary_check <- analyte_table_means %>% 
  filter(Study_ID == "Coombs & Melack, 2013")


coombs_check <- metadata_long %>% 
  filter(Study_ID == "Coombs & Melack, 2013")

```









```{r}
# This is another attempt to get what we are looking for 
control_impact_summary <- control_impact_fill %>% 
  group_by(Study_ID, Site, year, month) %>% 
  summarise(XeNO3 = mean(NO3_Interp[Burn_Unburn == "Burn"], na.rm = TRUE),
            XcNO3 = mean(NO3_Interp[Burn_Unburn == "Unburn"], na.rm = TRUE),
            XeDOC = mean(DOC_Interp[Burn_Unburn == "Burn"], na.rm = TRUE),
            XcDOC = mean(DOC_Interp[Burn_Unburn == "Unburn"], na.rm = TRUE))



  
gerla_month_mean_burn <- gerla_burn_fill %>%
  group_by(Climate, year, month) %>%
  summarize(Xe = mean(NO3[Burn_Unburn == "Burn"], na.rm = TRUE),
            TimeNorm_Burn = Xe * 6423/100)
```

```{r for loop attempt}
setwd("~/GitHub/rc_sfa-rc-3-wenas-meta/FireMeta_Rproj/inputs/Studies/meta_final")
# Step 1: Create a list of CSV file names
csv_files <- c("Abbott_et_al_2021.csv", "Burd_et_al_2018.csv", "Coombs_Melack_2013.csv")

# Step 2: Iterate through the list using a for loop
for (file_name in csv_files) {
  # Step 3: Read the CSV file into a data frame
  data <- read_csv(file_name) # Assuming you're using CSV files

  # Step 4: Fill time gaps in the data frame
  # Example: Assuming a "timestamp" column with time data
  # You can use functions from the tidyverse for this purpose
  data <- data %>%
    mutate(timestamp = mdy(Sampling_Date)) %>%  # Convert timestamp to a datetime object
    complete(timestamp = seq(min(timestamp), max(timestamp), by = "1 day")) %>%  # Fill time gaps
    fill(everything())  # Fill in missing values with the previous value

  # Step 5: Write the updated data frame back to a new CSV file
  # You can create a new file or overwrite the original file, depending on your needs
  # write_csv(data, file_name)  # Uncomment this line to overwrite the original file
  # write_csv(data, paste0("filled_", file_name))  # Uncomment this line to create a new file
}



```


```{r - code graveyard}
# # Fill gaps
# control_impact_gap_fill <- control_impact %>% 
#   group_by(Study_ID) %>% 
#   complete(Sampling_Date = seq.Date(min(Sampling_Date), max(Sampling_Date), by = "day")) %>% 
#   mutate(NO3 = na.approx(NO3))  
#   
# 
# df <- tibble(
#   Group = c("A", "A", "A", "B", "B"),
#   Date = as.Date(c("2023-10-01", "2023-10-02", "2023-10-04", "2023-10-01", "2023-10-03")),
#   Value = c(10, 15, 20, 5, 10)
# )
# 
# df_filled <- df %>%
#   group_by(Group) %>%
#   complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
#   fill(Value, .direction = "up")












```
